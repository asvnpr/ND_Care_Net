{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recommendation: wide_only.ipynb","provenance":[{"file_id":"1lKjJrVSV5tsz437zS7AuXg_cxDDcUohk","timestamp":1586557870381},{"file_id":"1GmBjHRLF0gIfjSjyT_AmpND4HpUP2yn6","timestamp":1585580865391}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMZZV7WRTDxt/neVZjzPhBW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JTuET-PkyoW9","colab_type":"text"},"source":["## Testing wide deep model example \n","\n","Following [original paper](https://arxiv.org/pdf/1606.07792.pdf) and primarily this [workshop](https://noelkonagai.github.io/Workshops/tensorflow_pt2_widedeep/). Additionally these resources: \n","- [how to build wide deep model w. keras and tf2] (https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b) and its [code](https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb)\n","-[feature crossing tutorial](https://developers.google.com/machine-learning/crash-course/feature-crosses/crossing-one-hot-vectors)\n","\n","Other implementations to look at:  \n","[1](https://github.com/Mohit67/Movie_recommendar/blob/master/wide_n_deep_tutorial.py),[2](https://github.com/wangby511/Recommendation_System), [3](https://github.com/AmoghM/Yelp-Restaurants-RecSys), [4](https://github.com/rajaharsha/Wide-Deep-Neural-Networks), [5](https://github.com/wang-henry4/wide-and-deep-recommender-model),[6](https://devblogs.nvidia.com/accelerating-wide-deep-recommender-inference-on-gpus/), [7](https://humboldt-wi.github.io/blog/research/information_systems_1718/08recommendation/),[8](https://www.youtube.com/watch?v=m_AZrITxs5M&t=0s)"]},{"cell_type":"code","metadata":{"id":"mfZAXRqirJ82","colab_type":"code","outputId":"9dfa49e2-e67b-4245-d1d8-92056d0064b9","executionInfo":{"status":"ok","timestamp":1586692705577,"user_tz":240,"elapsed":668584,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":352}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","prefix = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","!echo \"Project dir contents:\" && ls \"$prefix/\"\n","!echo -e \"\\nColab Notebook home dir:\" && ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","Project dir contents:\n","211_IN_Data_Parsing.ipynb    models\n","binder\t\t\t     Node_Text_Embedding_sBERT.ipynb\n","data\t\t\t     README.md\n","demos\t\t\t     semantic_embeddings_cluster_plot.ipynb\n","embeddings\t\t     service_embeddings_UMAP.ipynb\n","embed_serv_sim_search.ipynb  USE_embeddings.ipynb\n","figures\t\t\t     USE_embed_serv_sim_search.ipynb\n","Graph_Data_Extraction.ipynb\n","\n","Colab Notebook home dir:\n","drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GxqDBOntzIx1","colab_type":"code","outputId":"78854d43-d9ac-4736-c607-167153bf005e","executionInfo":{"status":"ok","timestamp":1586692718183,"user_tz":240,"elapsed":680951,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install -U ipykernel tensorflow-text==2.2.0rc2 plot-keras-history nest_asyncio"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting ipykernel\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/13/cc35fa7aef711d1b384ed8cf4304d1b16c38d8f91c1e8fb1466eae96f828/ipykernel-5.2.0-py3-none-any.whl (117kB)\n","\u001b[K     |████████████████████████████████| 122kB 3.5MB/s \n","\u001b[?25hCollecting tensorflow-text==2.2.0rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f7/0bf21f920ffbef71a521bd5187bd206c84ed7024c574717247878e7a4218/tensorflow_text-2.2.0rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 62.5MB/s \n","\u001b[?25hCollecting plot-keras-history\n","  Downloading https://files.pythonhosted.org/packages/2e/5a/8d60eae5d2624877fba35b63b70cea4edcf603c75883c004bb4715470d10/plot_keras_history-1.1.23.tar.gz\n","Collecting nest_asyncio\n","  Downloading https://files.pythonhosted.org/packages/fb/98/f4add7297a8d586fe7ac756707673d03d97b9ed7c171291bb43889dbfbc5/nest_asyncio-1.3.2-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: ipython>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.5.0)\n","Requirement already satisfied, skipping upgrade: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (4.3.3)\n","Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.3.4)\n","Requirement already satisfied, skipping upgrade: tornado>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (4.5.3)\n","Requirement already satisfied, skipping upgrade: tensorflow<2.3,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text==2.2.0rc2) (2.2.0rc2)\n","Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from plot-keras-history) (3.2.1)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from plot-keras-history) (1.0.3)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from plot-keras-history) (1.4.1)\n","Collecting sanitize_ml_labels\n","  Downloading https://files.pythonhosted.org/packages/64/f5/7904f0cb0146b8c9dd3d43c7a652be09be763d7c9fff76d05b8a01cef197/sanitize_ml_labels-1.0.9.tar.gz\n","Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (1.0.18)\n","Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (4.4.2)\n","Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (4.8.0)\n","Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (2.1.3)\n","Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (0.8.1)\n","Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (46.1.3)\n","Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n","Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (1.12.0)\n","Requirement already satisfied, skipping upgrade: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (4.6.3)\n","Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (19.0.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (2.8.1)\n","Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.3.3)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.2.0)\n","Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2.2.0)\n","Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2.2.0rc0)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.28.1)\n","Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.6.3)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.18.2)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.2.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.10.0)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.1.0)\n","Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2.10.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.34.2)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.1.0)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.12.1)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.9.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot-keras-history) (2.4.7)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot-keras-history) (1.2.0)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot-keras-history) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->plot-keras-history) (2018.9)\n","Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel) (0.1.9)\n","Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel) (0.6.0)\n","Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.6.0.post3)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.7.2)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.2.1)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.0.1)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2.21.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.4.1)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.2.8)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (4.0)\n","Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.1.1)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2.8)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (2020.4.5.1)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (1.3.0)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (0.4.8)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0rc0->tensorflow-text==2.2.0rc2) (3.1.0)\n","Building wheels for collected packages: plot-keras-history, sanitize-ml-labels\n","  Building wheel for plot-keras-history (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for plot-keras-history: filename=plot_keras_history-1.1.23-cp36-none-any.whl size=6405 sha256=0e8b81fb901da49ae27c5c13daaa97da6fc9253a2a1ffd8869707126ef780f81\n","  Stored in directory: /root/.cache/pip/wheels/c0/78/33/da5ed769fab5587fcdae95271e8d19106e3b92b3ae2d46382d\n","  Building wheel for sanitize-ml-labels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sanitize-ml-labels: filename=sanitize_ml_labels-1.0.9-cp36-none-any.whl size=6476 sha256=49b644dd40e2864dff260a185baac130dbea2a4b19fc690b52f0331937aeb514\n","  Stored in directory: /root/.cache/pip/wheels/9d/e9/ec/ca775287a4d0f9e87a56220bf82ca01d9cff045026271fad24\n","Successfully built plot-keras-history sanitize-ml-labels\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: ipykernel, tensorflow-text, sanitize-ml-labels, plot-keras-history, nest-asyncio\n","  Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","Successfully installed ipykernel-5.2.0 nest-asyncio-1.3.2 plot-keras-history-1.1.23 sanitize-ml-labels-1.0.9 tensorflow-text-2.2.0rc2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["ipykernel"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"nB0OorxHziCk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"50a1f99c-d11d-47a2-d123-3be31997a489","executionInfo":{"status":"ok","timestamp":1586692721058,"user_tz":240,"elapsed":683641,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}}},"source":["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"Download and clean the Census Income Dataset.\"\"\"\n","\n","import argparse\n","import os\n","import shutil\n","\n","try:\n","    %tensorflow_version 2.x\n","except Exception as e:\n","    print(e)\n","import tensorflow as tf\n","\n","import pandas as pd\n","import json\n","import csv\n","import pickle\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import tensorflow_hub as hub\n","import random\n","import multiprocessing\n","import ast\n","from sklearn.model_selection import train_test_split\n","import tensorflow_text\n","\n","import dask.dataframe as dd\n","from dask.distributed import Client, LocalCluster\n","\n","cluster = LocalCluster(processes=False)\n","client = Client(cluster)\n","client"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/distributed/bokeh/core.py:57: UserWarning: \n","Failed to start diagnostics server on port 8787. [Errno 99] Cannot assign requested address\n","  warnings.warn('\\n' + msg)\n","/usr/local/lib/python3.6/dist-packages/distributed/deploy/local.py:197: UserWarning: \n","Could not launch service 'bokeh' on port 8787. Got the following message:\n","\n","[Errno 99] Cannot assign requested address\n","  self.scheduler.start(scheduler_address)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<table style=\"border: 2px solid white;\">\n","<tr>\n","<td style=\"vertical-align: top; border: 0px solid white\">\n","<h3>Client</h3>\n","<ul>\n","  <li><b>Scheduler: </b>inproc://172.28.0.2/532/1\n","</ul>\n","</td>\n","<td style=\"vertical-align: top; border: 0px solid white\">\n","<h3>Cluster</h3>\n","<ul>\n","  <li><b>Workers: </b>1</li>\n","  <li><b>Cores: </b>40</li>\n","  <li><b>Memory: </b>37.96 GB</li>\n","</ul>\n","</td>\n","</tr>\n","</table>"],"text/plain":["<Client: scheduler='inproc://172.28.0.2/532/1' processes=1 cores=40>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Hle4R8av5fxy","colab_type":"code","outputId":"f8519c6f-40a7-4f13-ff45-5b018566be56","executionInfo":{"status":"ok","timestamp":1586692725993,"user_tz":240,"elapsed":688477,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":758}},"source":["try: \n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    # This is the TPU initialization code that has to be at the beginning.\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","except Exception as e:\n","    print(e)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.60.239.82:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.60.239.82:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EtfxEpCe2DB-","colab_type":"text"},"source":["#### Default Args and names"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7GTNX_xS_YD3","outputId":"5c012894-6dfc-4630-d8ae-f4024aacc868","executionInfo":{"status":"ok","timestamp":1586692727179,"user_tz":240,"elapsed":689594,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# tf.enable_eager_execution()\n","# PREFIX = '/afs/crc.nd.edu/user/a/aveganog/ND_Care_Net'\n","PREFIX = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","TRAINING_FILE = 'care-net.train'\n","EVAL_FILE = 'care-net.test'\n","\n","NODE_TYPE = 'services'\n","\n","# needed for file name below\n","MODEL = 'USE'\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n","MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n","MODEL_TYPE = MODEL_URL.split('/')[-2]\n","MODEL_VER = MODEL_URL.split('/')[-1]\n","\n","!mkdir -p \"$prefix/embeddings/$MODEL/\"\n","print(\"Using embeddings from Model {}_{}_v{}\".format(MODEL, MODEL_TYPE, MODEL_VER))\n","\n","DATA_DIR = os.path.join(PREFIX, 'data', 'wide_deep_dataset_chunks')\n","\n","# help = 'Base directory for the model.')\n","# MODEL_TYP = 'wide-deep-USE'\n","MODEL_TYP = 'deep-USE'\n","# MODEL_TYP = 'wide-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', MODEL_TYP)\n","\n","# help = 'Number of training epochs.')\n","TRAIN_EPOCHS = 5\n","\n","# help = 'The number of training epochs to run between evaluations.')\n","EPOCHS_PER_EVAL =2 \n","\n","# help = 'Number of examples per batch.')\n","BATCH_SIZE = 256\n","QBAT_SIZE = 512\n","\n","# parser.add_argument(\n","#     '--train_data', type=str, default='/tmp/census_data/adult.data',\n","#     help='Path to the training data.')\n","\n","# parser.add_argument(\n","#     '--test_data', type=str, default='/tmp/census_data/adult.test',\n","#     help='Path to the test data.')\n","\n","TRAIN_PCNT = 0.7 \n","TEST_PCNT = 1 - TRAIN_PCNT\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Label'\n","]\n","\n","CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0], [''], [0], [0], [0]]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using embeddings from Model USE_universal-sentence-encoder_v4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W3e8OjOnStm6","colab_type":"code","colab":{}},"source":["\"\" # download auxillary data\n","# tf.enable_eager_execution()\n"," # json nodes of 211 IN services\n","with open(PREFIX + '/data/services_nodes.json') as sn:\n","    serv_nodes = json.loads(sn.read())\n","\n","# our heterogeneous information network. \n","# we're only using the services data\n","with open(PREFIX + '/data/HIN_nodes.json') as taxo:\n","    hin_nodes = json.loads(taxo.read())\n","\n","# map service_ids to node numbers in our graph\n","with open(PREFIX + '/data/service_id_to_node_num.json') as sn:\n","    serv_trans = json.loads(sn.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/code_to_node_num.json') as ct:\n","    code_trans = json.loads(ct.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/tagged_texts.json') as ct:\n","    tagged_texts = json.loads(ct.read())\n","\n","# load list of queries/keywords from taxonomy data\n","queries_path = os.path.join(PREFIX, 'data', 'HIN_references.csv')\n","with open(queries_path) as qf:\n","  queries = qf.read()\n","  queries = queries.split(',')\n","  # cleanup some bad data. TODO: Fix in notebook that generates the data\n","  queries = [''] + [q for q in queries if q != '']\n","\n","with open(os.path.join(PREFIX, 'data', 'service_recommendations.json')) as rf:\n","    tagged_recs = json.loads(rf.read())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"roKW2lt51vGV","colab_type":"text"},"source":["### Now we can build, run, and eval the test wide deep model"]},{"cell_type":"markdown","metadata":{"id":"3NUaeCiBFOC9","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"sb51fu2p2Fxo","colab_type":"text"},"source":["#### Build input feature columns"]},{"cell_type":"code","metadata":{"id":"TnIMz9DJ14ay","colab_type":"code","colab":{}},"source":["def build_model_columns():\n","    \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n","    # Continuous columns\n","    distance = tf.feature_column.numeric_column('Distance', shape=(1,), dtype=tf.float32)\n","    similarity = tf.feature_column.numeric_column('Embedding_Similarity', shape=(1,))\n","    # month of referral\n","    month = tf.feature_column.categorical_column_with_hash_bucket('Month', hash_bucket_size=13, dtype=tf.string)\n","    month = tf.feature_column.indicator_column(month)\n","\n","    #   prev_recs = tf.feature_column.numeric_column(key='Previously_Recommended',shape=(16547,))\n","    voc_list = [str(serv_nodes[nid]['serv_id']) for nid in serv_nodes]\n","    prev_recs = tf.feature_column.categorical_column_with_vocabulary_list(key='Previously_Recommended', vocabulary_list=voc_list, dtype=tf.string)\n","    prev_recs = tf.feature_column.embedding_column(prev_recs,32)\n","    serv_id = tf.feature_column.categorical_column_with_hash_bucket('Selected_Service_ID', hash_bucket_size=20000)\n","    serv_id = tf.feature_column.indicator_column(serv_id)\n","    bucket_sim = tf.feature_column.bucketized_column(similarity, [0.2,0.4,0.6,0.8])\n","    cross = tf.feature_column.crossed_column([bucket_sim, 'Selected_Service_ID'], hash_bucket_size=10000)\n","    # try this later to combine embeddings into one feature:\n","    # https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings\n","    \n","    # Wide columns and deep columns.\n","    base_columns = [\n","        distance, similarity, month#, prev_recs\n","    ]\n","\n","    crossed_columns = [\n","        tf.feature_column.indicator_column(cross)\n","    ]\n","\n","    wide_columns = base_columns + [serv_id] #+ crossed_columns #+ [bucket_sim]\n","\n","    return wide_columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZ_sH_342LzV","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"VTsMoXuC3JT9","colab_type":"code","colab":{}},"source":["data_files = os.listdir(DATA_DIR)\n","data_files = [os.path.join(DATA_DIR, f) for f in data_files]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qq4yQUGquCrQ","colab":{}},"source":["nid_idxs = {}\n","for i, nid in enumerate(hin_nodes):\n","    nid_idxs[nid] = i\n","\n","# save indexes of services to use in our dataset for embedding lookup\n","sid_idxs = {}\n","for i, sid in enumerate(serv_trans):\n","    sid_idxs[sid] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvFKWwpZWdZL","colab_type":"code","colab":{}},"source":["max_rec_len = 0\n","\n","def preproc_query(emb_key):\n","    # idx = None\n","    # if str(emb_key) == 'nan':\n","    #     emb_key = ''\n","    if emb_key in q_idxs:\n","        idx = q_idxs[str(emb_key)]\n","    else:\n","        if str(emb_key) not in tagged_embeds:\n","            tagged_embeds[str(emb_key)] = extract_embed([str(emb_key)])\n","        q_idxs[str(emb_key)] = len(tagged_embeds.keys()) - 1\n","        idx = q_idxs[str(emb_key)]\n","    return idx\n","\n","def get_recs(sid):\n","    return tagged_recs[str(sid).split('.')[0]]\n","\n","def proc_recs(row):\n","    return shared_recs\n","\n","def preproc_lbl(lbl):\n","    print(int(lbl))\n","    return int(lbl)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ejy2HiGhm6m4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGeet-Dw9Yad","colab_type":"code","colab":{}},"source":["CSV_COLUMNS_OG = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Shared_Recommendations',\n","            'Query',\n","            'Label'\n","]\n","\n","DEEP_COLUMNS = [\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Query'\n","]\n","\n","WIDE_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Shared_Recommendations'\n","]\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Shared_Recommendations',\n","            'Query',\n","            'Label'\n","]\n","\n","from tqdm import tqdm\n","from sklearn import preprocessing\n","\n","LABEL_NAME = CSV_COLUMNS[-1]\n","serv_ids = None\n","def load_df(files, has_checkpoints=False):\n","\n","    data_df = []\n","    tot = len(files)\n","    # for i, f in enumerate(files):\n","    #     fname = f.split('/')[-1]\n","    #     # skip hidden files\n","    #     if fname[0] == '.':\n","    #         continue\n","    #     if has_checkpoints:\n","    #         data_df.append(pd.read_parquet(f))\n","    #     else:\n","    # None means dask uses a block per file\n","    data_df = dd.read_csv(os.path.join(DATA_DIR, 'wide*.csv'), blocksize=None).head(n=2000)\n","            # data_df.append(pd.read_csv(f, nrows=2500))\n","    # print(\"loaded file {} {} of {}\".format(fname, i+1, tot))\n","    # data_df = pd.concat(data_df)\n","\n","    if not has_checkpoints:\n","        # ignore indexing column\n","        data_df.columns = CSV_COLUMNS_OG\n","        # data_df.compute()\n","        # data_df = data_df.replace(np.nan, '', regex=True)\n","        # data_df[CSV_COLUMNS[0]] = data_df[CSV_COLUMNS[0]].fillna(-1.0)#, inplace=True)\n","        # data_df[CSV_COLUMNS[1]] = data_df[CSV_COLUMNS[1]].fillna(0.0)#, inplace=True)\n","        # data_df[CSV_COLUMNS[2]] = data_df[CSV_COLUMNS[2]].fillna(0.0)#, inplace=True)\n","        # data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].fillna('')#, inplace=True)\n","        # data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].fillna('')#, inplace=True)\n","        # data_df[CSV_COLUMNS[5]] = data_df[CSV_COLUMNS[5]].fillna('')#, inplace=True)\n","        # data_df[CSV_COLUMNS[6]] = data_df[CSV_COLUMNS[6]].fillna('')#, inplace=True)\n","        \n","        data_df.fillna(-1)\n","        print(\"Dropped NA vals\")\n","        tqdm.pandas()\n","        print(\"Applying funcs to cols\")\n","        serv_ids = data_df[CSV_COLUMNS[3]].apply(lambda x: hin_nodes[str(x)]['serv_id'])\n","        serv_id_recs = serv_ids.progress_apply(lambda x: get_recs(x))\n","        candidate_ids = data_df[CSV_COLUMNS[4]].apply(lambda x: hin_nodes[str(x)]['serv_id'])\n","        candidate_id_recs = candidate_ids.progress_apply(lambda x: get_recs(x))\n","        shared_recs = []\n","        pbar = tqdm(total=len(serv_ids), mininterval=15, desc='Finding shared recommendations')\n","        for i in range(len(serv_ids)):\n","            shared_recs.append(np.logical_or(serv_id_recs.iloc[i], candidate_id_recs.iloc[i]))\n","            pbar.update(1)\n","        \n","        z_score_scaler = preprocessing.StandardScaler()\n","        data_df[CSV_COLUMNS[0]] = z_score_scaler.fit_transform(data_df[CSV_COLUMNS[0]].values.reshape(-1,1))\n","        # data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].progress_apply(lambda f: preproc_serv(f))\n","        # data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].progress_apply(lambda f: preproc_serv(f))\n","        # data_df[CSV_COLUMNS[5]] = list(data_df[CSV_COLUMNS[5]].apply(lambda f: preproc_rec(f)))\n","        data_df[CSV_COLUMNS[5]] = shared_recs\n","        \n","        # data_df = data_df[CSV_COLUMNS]\n","        labels = data_df.pop('Label')\n","        CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","        data_df[CSV_COLUMNS[-1]] = np.array(serv_ids.values).astype(np.int32)\n","        # data_df = data_df[DEEP_COLUMNS]\n","        data_df = data_df[WIDE_COLUMNS + ['Selected_Service_ID']]\n","        # data_df = data_df[WIDE_COLUMNS]\n","    \n","    return train_test_split(data_df.values, labels.values, train_size=0.8, random_state=19, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-unuEDewVX_Z","colab_type":"code","outputId":"24c70472-818c-4b67-8ea2-b692aa70062c","executionInfo":{"status":"ok","timestamp":1586693125226,"user_tz":240,"elapsed":5958,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":147}},"source":["import glob\n","\n","has_checkpoints = False\n","X_train, X_val, y_train, y_val = load_df(data_files, has_checkpoints)\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=(0.6/0.8), random_state=19, shuffle=True)\n","# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.7/0.9, random_state=19, shuffle=True)\n","print(\"Using train data with shape: {}\".format(X_train.shape))\n","print(\"Using test data with shape: {}\".format(X_test.shape))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["100%|██████████| 2000/2000 [00:00<00:00, 546417.93it/s]\n","100%|██████████| 2000/2000 [00:00<00:00, 421114.86it/s]\n","Finding shared recommendations:   0%|          | 0/2000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Dropped NA vals\n","Applying funcs to cols\n","Using train data with shape: (1199, 5)\n","Using test data with shape: (401, 5)\n"],"name":"stdout"},{"output_type":"stream","text":["\rFinding shared recommendations: 100%|██████████| 2000/2000 [00:04<00:00, 408.91it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jiwx55CU-nuD","colab_type":"code","outputId":"e728cf07-aca9-476d-93c5-b97ad38e0ce3","executionInfo":{"status":"error","timestamp":1586695468467,"user_tz":240,"elapsed":836,"user":{"displayName":"Alejandro Vega-Nogales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK-9GeMyuc3khjD9AgjWA9fZChJhwiaeFfE1Kk=s64","userId":"06234555604493977329"}},"colab":{"base_uri":"https://localhost:8080/","height":401}},"source":["import dask.array as da\n","dtypes = [np.float32,np.float32,str,str,str,np.array([]),str,str]\n","dtype_ex = pd.Series([0.0,0.0,'','','',[],'',''])\n","\n","X_tr = dd.from_dask_array(da.from_array(X_train, chunks=(2000,8)))\n","X_tr.columns = WIDE_COLUMNS + ['Selected_Service_ID']\n","X_tr['Shared_Recommendations'] = X_tr['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","X_vl = dd.from_dask_array(da.from_array(X_val))\n","X_vl.columns = WIDE_COLUMNS + ['Selected_Service_ID']\n","X_vl['Shared_Recommendations'] = X_vl['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32))\n","X_te = dd.from_dask_array(da.from_array(X_test))\n","X_te.columns = WIDE_COLUMNS + ['Selected_Service_ID']\n","X_te['Shared_Recommendations'] = X_te['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32))\n","\n","print(\"Using train dataset with shape: {}\".format(X_train.shape))\n","print(\"Using validation dataset with shape: {}\".format(X_val.shape))\n","print(\"Using test dataset with shape: {}\".format(X_test.shape))\n","X_tr.head()"],"execution_count":30,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-9d09af3425a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWIDE_COLUMNS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Selected_Service_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shared_Recommendations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shared_Recommendations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_ex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_vl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dask_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_vl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWIDE_COLUMNS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Selected_Service_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_vl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shared_Recommendations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_vl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shared_Recommendations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dask/array/core.py\u001b[0m in \u001b[0;36mfrom_array\u001b[0;34m(x, chunks, name, lock, asarray, fancy, getitem, meta)\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m     chunks = normalize_chunks(\n\u001b[0;32m-> 2753\u001b[0;31m         \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2754\u001b[0m     )\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dask/array/core.py\u001b[0m in \u001b[0;36mnormalize_chunks\u001b[0;34m(chunks, shape, limit, dtype, previous_chunks)\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2474\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dask/array/core.py\u001b[0m in \u001b[0;36mauto_chunks\u001b[0;34m(chunks, shape, limit, dtype, previous_chunks)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         raise NotImplementedError(\n\u001b[0;32m-> 2558\u001b[0;31m             \u001b[0;34m\"Can not use auto rechunking with object dtype. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2559\u001b[0m             \u001b[0;34m\"We are unable to estimate the size in bytes of object data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         )\n","\u001b[0;31mNotImplementedError\u001b[0m: Can not use auto rechunking with object dtype. We are unable to estimate the size in bytes of object data"]}]},{"cell_type":"code","metadata":{"id":"-Z5SUTduRVUu","colab_type":"code","colab":{}},"source":["CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Selected_Service_ID'\n","]\n","\n","# following: https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n","\n","# initialize inputs for model with appropriate keras layers\n","wide_in = {}\n","for i in range(len(CSV_COLUMNS)):\n","    if i in (0,1):\n","        print(\"Wide Input: Added {} input layer as float32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.float32)\n","    elif i == 5:\n","        print(\"Wide Input: Added {} input layer as list of strings\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(16547,), dtype='string')\n","        # inputs[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(16547,), dtype='int32')\n","    elif i in (2,6):\n","        print(\"Wide Input: Added {} input layer as string\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.string)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bn0DglpYv9pv","colab_type":"code","colab":{}},"source":["print(wide_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8zSC4fPVuMG","colab_type":"code","colab":{}},"source":["# import random\n","MODEL_DIR = os.path.join(PREFIX, 'models', MODEL_TYP, \"init_baseline_deep-only\")#.format(BATCH_SIZE)) #+ str(random.randint(1,1001))\n","print(\"Using model dir {}\".format(MODEL_DIR))\n","\n","config = tf.estimator.RunConfig(model_dir=MODEL_DIR, tf_random_seed=42, save_summary_steps=100,\n","    save_checkpoints_steps=250, session_config=None, keep_checkpoint_max=50, \n","    keep_checkpoint_every_n_hours=1,session_creation_timeout_secs=7200)\n","\n","def rec_line_gen(R):\n","    for r in range(len(R)):\n","        print(R.shape)\n","        print(r)\n","        yield np.asarray(R[r])\n","\n","def wide_input_generator(X,y):\n","    while True:\n","        X = np.array(X)\n","        for i in range(len(X)):\n","            # WIDE: \n","            x1,x2,x3,x4 = X[:,i]\n","            yield np.float32(x1),np.float32(x2),str(x3),str(x4),y[i]\n","\n","def wide_input_fn(X, y, shuffle=True):\n","\n","    # # WIDE ONLY\n","    def parse_line(f1,f2,f3,f4,f5):\n","        cols = [f1,f2,f3,f4,f5]\n","        lbl = cols.pop(-1)\n","        labels = {'Pred': lbl}\n","        feats = dict(zip(WIDE_COLUMNS + ['Selected_Service_ID'], cols))\n","        return feats,labels\n","\n","    dist = X['Distance'].values.astype(np.float32)\n","    sim = X['Embedding_Similarity'].values.astype(np.float32)\n","    mon = X['Month'].apply(lambda x: str(x).split('.')[0]).values.astype('U1')\n","    recs = X['Shared_Recommendations']\n","    recs = []\n","    pbar = tqdm(total=len(X), mininterval=10, desc=\"Converting sparse arrays to TF Dataset...\")\n","    for i in range(len(X)):\n","        recs.append(list(X_tr['Shared_Recommendations'].iloc[i].to_dense()))\n","        pbar.update(1)\n","    sid = X['Selected_Service_ID'].apply(lambda x: str(x).split('.')[0]).values.astype('U6')\n","\n","    ds1 = tf.data.Dataset.from_tensor_slices(dist)\n","    ds2 = tf.data.Dataset.from_tensor_slices(sim)\n","    ds3 = tf.data.Dataset.from_tensor_slices(mon)\n","    ds6 = tf.data.Dataset.from_tensors(recs)\n","    ds8 = tf.data.Dataset.from_tensor_slices(sid)\n","    ds9 = tf.data.Dataset.from_tensor_slices(y)\n","    \n","    dataset = tf.data.Dataset.zip((ds1,ds2,ds3,ds6,ds8,ds9))\n","    dataset = dataset.map(parse_line, num_parallel_calls=4)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(X))\n","        dataset = dataset.repeat(TRAIN_EPOCHS)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    # dataset = dataset.cache() \n","    return dataset\n","\n","ds = wide_input_fn(X_tr, y_train)\n","X_tr.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-W1u9B5V7PB","colab_type":"code","colab":{}},"source":["tf.data.Dataset.from_tensor_slices(list(X_tr['Previously_Recommended'].values.reshape(len(X_tr),1)[:5]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBuENTrDTF57","colab_type":"code","colab":{}},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","\n","wide_columns = build_model_columns()\n","# following: https://blog.tensorflow.org/2018/04/predicting-price-of-wine-with-keras-api-tensorflow.html\n","def make_wide_model(linear_columns, linear_inputs):\n","    wide = layers.DenseFeatures(linear_columns)(linear_inputs)\n","    pred = layers.Dense(1, activation='sigmoid', name='Pred')(wide)\n","    wide_model = Model(inputs=list(wide_in.values()), outputs=pred)\n","    wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(l1_regularization_strength=0.5, l2_regularization_strength=0.5), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","    return wide_model\n","\n","wide_model = make_wide_model(wide_columns, wide_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0pQm7IGWWzV","colab_type":"code","colab":{}},"source":["wide_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkBeTEGMbuDv","colab_type":"code","colab":{}},"source":[" tf.keras.utils.plot_model(wide_model, os.path.join(PREFIX, 'figures', 'deep_keras_model.png'), show_shapes=True, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1cWazdK3kK1","colab_type":"code","colab":{}},"source":["model_estimator = tf.keras.estimator.model_to_estimator(wide_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpMUnZ-VaqwP","colab_type":"code","colab":{}},"source":["train_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    if col in ('Month', 'Selected_Service_ID'):\n","        train_dict[col] = X_tr[col].values.astype('U6')\n","        print(train_dict[col].dtype)\n","    else:\n","        train_dict[col] = X_tr[col].values\n","\n","val_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    if col in ('Month', 'Selected_Service_ID'):\n","        val_dict[col] = X_vl[col].values.astype('U6')\n","        print(val_dict[col].dtype)\n","    else:\n","        val_dict[col] = X_vl[col].values\n","\n","# with strategy.scope():\n","    # needs to be created inside here for TPU to work\n","wide_model = make_wide_model(wide_columns, wide_in)\n","train_ds = wide_input_fn(X_tr, y_train)\n","wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","\n","model_hist = wide_model.fit(x=list(train_dict.values()), y=y_train, epochs=TRAIN_EPOCHS, verbose=10, validation_data=(list(val_dict.values()), y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj8EgobRELIl","colab_type":"code","colab":{}},"source":["from plot_keras_history import plot_history\n","import matplotlib.pyplot as plt\n","\n","# plot keras history metrics \n","plot_history(model_hist.history)\n","plt.show()\n","plot_history(model_hist.history, path=\"standard.png\")\n","plt.close()\n","\n","# follow for full model report\n","# https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n","# https://www.machinecurve.com/index.php/2019/10/08/how-to-visualize-the-training-process-in-keras/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1j8xmV0bnIE","colab_type":"code","colab":{}},"source":["feed_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    feed_dict[col] = X_te[col].values\n","# feed_dict.update({'Pred': y_train})\n","wide_model.evaluate(x=list(feed_dict.values()), y=y_test, batch_size=BATCH_SIZE, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9VnsMLYdQ7l","colab_type":"code","colab":{}},"source":["MODEL_TYP = 'wide-deep-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', 'wide_and_deep_{}'.format(MODEL), MODEL_TYP)\n","model_fpath = os.path.join(MODEL_DIR, '{}-model.h5'.format(MODEL_TYP)\n","deep_model.save(model_fpath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hU0xWRDVqbW","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir \"$MODEL_DIR/\" #--debugger_port 6969 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8l-RCB_0aW2I","colab_type":"code","colab":{}},"source":["train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(X_te, y_test), name='wide-deep-care-net_batch-size_{}'.format(BATCH_SIZE),\n","                                  start_delay_secs=2, throttle_secs=10)\n","evals, exports = tf.estimator.train_and_evaluate(model_estimator, train_spec, eval_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MIFUYocrONQ","colab_type":"code","colab":{}},"source":["qq    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(X_te, y_test), name='wide-deep-care-net_batch-size_{}'.format(BATCH_SIZE),\n","                                  start_delay_secs=2, throttle_secs=10)\n","evals, exports = tf.estimator.train_and_evaluate(wide_deep, train_spec, eval_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsvjtg-FtKcK","colab_type":"code","colab":{}},"source":["X_tr['Selected_Service_Embedding'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zI5maBHdyBbv","colab_type":"code","colab":{}},"source":["import math\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","# train_ds = input_fn(X_tr, y_train)\n","# test_ds = input_fn(X_te, y_test, shuffle=False)\n","num_5k_steps = math.ceil(len(X_te) / 5000) \n","train_steps = math.ceil(len(X_tr) / num_5k_steps)\n","\n","accs, precs, recs, f1s = [],[],[],[]\n","for i in range(num_5k_steps):\n","    print('='*20, \"Train/Eval round {}/{}\".format(i,num_5k_steps), '='*20)\n","    eval_er.train(input_fn=lambda: input_fn(X_tr, y_train), steps=train_steps)\n","\n","    start_idx = i * train_steps\n","    end_idx = start_idx + train_steps\n","    print(\"Using train samples {} to {}\".format(start_idx,end_idx))\n","    X_metr = None\n","    preds = None\n","    y_metr = None\n","    if end_idx <= len(X_train):\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx:end_idx]\n","        print(\"Using subset of train data with size {}\".format(len(X_metr)))\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx:end_idx]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    else:\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx::]\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx::]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    yhat = []\n","    for i in range(train_steps):\n","        pred = next(preds)\n","        print(pred.keys())\n","        yhat.append(pred['Pred'])\n","    print(\"Going through {} predictions for training subset\".format(len(yhat)))\n","    yhat_classes = list(map(lambda x: np.round(x),yhat))\n","    # accuracy: (tp + tn) / (p + n)\n","    accuracy = accuracy_score(y_metr, yhat_classes)\n","    print('Accuracy: %f' % accuracy)\n","    # precision tp / (tp + fp)\n","    precision = precision_score(y_metr, yhat_classes)\n","    print('Precision: %f' % precision)\n","    # recall: tp / (tp + fn)\n","    recall = recall_score(y_metr, yhat_classes)\n","    print('Recall: %f' % recall)\n","    # f1: 2 tp / (2 tp + fp + fn)\n","    f1 = f1_score(y_metr, yhat_classes)\n","    print('F1 score: %f' % f1)\n","    accs.append(accuracy)\n","    precs.append(precision)\n","    recs.append(recall)\n","    f1s.append(f1)\n","\n","    eval_er.evaluate(input_fn=lambda: input_fn(X_te, y_test, shuffle=False), steps=5000, name='5kEvals')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GR-lfvR_1up","colab_type":"code","colab":{}},"source":["    cfname = os.path.join(MODEL_DIR, 'label_metadata.tsv')\n","with open(fname, 'w') as f:\n","    f.write(\"Index\\tLabel\\tRecommended\\n\")\n","    sids = list(tagged_embeds.keys())\n","    for idx in range(len(sids)):\n","        typ = None\n","        if sids[idx] in hin_nodes:\n","            name = hin_nodes[sids[idx]]['name']\n","            typ = 'Service'\n","        else:\n","            name = sids[idx]\n","            typ = 'Query'\n","        # sel_name = serv_nodes[sel_idx]['name']\n","        # cand_idx = str(int(X_t['Candidate_Service_Embedding'].values[idx]))\n","        # try:\n","            # cand_name = serv_nodes[cand_idx]['name']\n","        # except:\n","            # continue\n","        # name = \"{}-{}\".format(sel_name, cand_name)\n","        f.write(\"{}\\t{}\\t{}\\n\".format(idx, name, typ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLTXPnxjaK-B","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# yhat = np.array([tf.argmax(pred['Pred'],1) for pred in preds])[:,0]\n","yhat = []\n","for pred in preds:\n","    yhat.append(pred['Pred'])\n","yhat_classes = list(map(lambda x: np.round(x),yhat))\n","\n","\n","# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(y_test, yhat_classes)\n","print('Accuracy: %f' % accuracy)\n","# precision tp / (tp + fp)\n","precision = precision_score(y_test, yhat_classes)\n","print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_test, yhat_classes)\n","print('Recall: %f' % recall)\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(y_test, yhat_classes)\n","print('F1 score: %f' % f1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"odCFG6HKQ-zx","colab_type":"code","colab":{}},"source":["import datetime\n","X_t = pd.DataFrame(X_test)\n","X_t.columns = CSV_COLUMNS\n","# X_t.columns = DEEP_COLUMNS\n","# X_t.columns = WIDE_COLUMNS #+ ['Selected_Service_ID']\n","\n","\n","preds = eval_er.evaluate(lambda: input_fn(X_t, y_test,shuffle=False), name='evalShuffled')\n","# preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# print(next(preds))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hh6r6hDtdEFv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVyEaLfOrnNp","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"AlBSM3DG_ClO","colab_type":"code","colab":{}},"source":["    !rm -rf MODEL_DIR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"reOkfht5yzyY","colab_type":"code","colab":{}},"source":["print(\"Loading model as keras model from {}\".format(export_path))\n","export_path = os.path.join('/tmp/census_model', \"1585107419\")\n","keras_model = tf.keras.models.load_model(export_path, compile=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2YQj0gN3tA0","colab_type":"code","colab":{}},"source":["print(keras_model.asset_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l82RDk-ApgrZ","colab_type":"code","colab":{}},"source":["keras_model.tensorflow_version\n","tf.keras.utils.plot_model(keras_model, 'census_test_model.png', show_shapes=False, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4vTaKAyVLqu","colab_type":"code","colab":{}},"source":["!ls \"$export_path/../\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok6gHp-Gzwoo","colab_type":"code","colab":{}},"source":["# FLAGS, unparsed = parser.parse_known_args()\n","# tf.app.run(argv=[sys.argv[0]] + unparsed)\n","\n","# if not tf.compat.v1.gfile.Exists(FLAGS.data_dir):\n","#     tf.compat.v1.gfile.MkDir(FLAGS.data_dir)\n","\n","# data_df = pd.DataFrame()\n","# data_df, labels = load_df(DATA_DIR)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2wvMcd5b7xY","colab_type":"code","colab":{}},"source":["# dataset = complete_dataset(data_df)\n","# TEST_PCT = 0.3\n","# test_size = int(TEST_PCT * len(dataset))\n","# train_size = int(len(dataset) - test_size)\n","\n","# test_data = dataset.take(test_size)\n","# train_data = dataset.skip(test_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cxGgku62SPs","colab_type":"code","colab":{}},"source":["# def build_estimator(model_dir, model_type):\n","#   \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n","#   wide_columns, deep_columns = build_model_columns()\n","#   hidden_units = [100, 75, 50, 25]\n","\n","#   # Create a tf.estimator.RunConfig to ensure the model is run on GPU\n","#   run_config = tf.estimator.RunConfig().replace(\n","#       session_config=tf.compat.v1.ConfigProto(device_count={'GPU': 1}))\n","\n","#   if LEARN_TYPE == 'wide':\n","#     return tf.estimator.LinearClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=wide_columns,\n","#         config=run_config)\n","#   elif LEARN_TYPE == 'deep':\n","#     return tf.estimator.DNNClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=deep_columns,\n","#         hidden_units=hidden_units,\n","#         config=run_config)\n","#   else:\n","#     return tf.estimator.DNNLinearCombinedClassifier(\n","#         model_dir=model_dir,\n","#         linear_feature_columns=wide_columns,\n","#         dnn_feature_columns=deep_columns,\n","#         dnn_hidden_units=hidden_units,\n","#         config=run_config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eI4sWLGg2cxg","colab_type":"code","colab":{}},"source":["def input_fn(data_files, num_epochs, shuffle, batch_size):\n","#     \"\"\"Generate an input function for the Estimator.\"\"\"\n","#     # assert tf.compat.v1.gfile.Exists(data_file), (\n","#     #     '%s not found. Please make sure you have either run data_download.py or '\n","#     #     'set both arguments --train_data and --test_data.' % data_file)\n","\n","#     def parse_dataset_lines(value):\n","#         # print('Parsing dataset...')\n","#         columns = tf.io.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS, select_cols=[0,1,2,3,4,6,7])\n","#         print(columns)\n","#         print()\n","#         # refactor our data. TODO change this directly when creating dataset\n","#         lbl = columns.pop(-1)\n","#         labels = {'Label': tf.py_function(func=preproc_lbl,\n","#                                                 inp=[lbl],\n","#                                                 Tout=tf.int32)}\n","#         # labels = lbl\n","#         CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","#         columns = columns + [columns[3]]\n","#         print(\"Pass1\")\n","#         features = dict(zip(CSV_COLUMNS, columns))\n","#         features.pop(CSV_COLUMNS[5])\n","#         features[CSV_COLUMNS[-1]] = columns[3]\n","#         print(\"Pass2\")\n","#         for i in (3,4):\n","#             feat = features[CSV_COLUMNS[i]]\n","#             features[CSV_COLUMNS[i]] = tf.py_function(func=preproc_serv,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_query,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         print(features)\n","#         # features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_rec,\n","#                                                             #  inp=[features[CSV_COLUMNS[5]]],\n","#         print(\"Pass3\")                                        #  Tout=(tf.int32))\n","#         return features, labels\n","        \n","#     # Extract lines from input files using the Dataset API.\n","#     # data_files = tf.io.matching_files(tf.convert_to_tensor(data_files))\n","    \n","#     dfs = tf.data.Dataset.from_tensor_slices(data_files)\n","#     # dataset = tf.data.TextLineDataset(data_files)\n","#     dataset = tf.data.TextLineDataset(data_files)\n","#     print(dataset)\n","#     if shuffle:\n","#         dataset = dataset.shuffle(buffer_size=TRAIN_SIZE)\n","#     dataset = dataset.map(parse_dataset_lines, num_parallel_calls=multiprocessing.cpu_count())\n","#     print(\"Pass4\")\n","#     # dataset = dataset.map(fn, num_parallel_calls=multiprocessing.cpu_count())\n","#     # dataset = dataset.map(parse_dataset, num_parallel_calls=multiprocessing.cpu_count())\n","\n","#     # We call repeat after shuffling, rather than before, to prevent separate\n","#     # epochs from blending together.\n","#     print(dataset)\n","#     dataset = dataset.repeat(TRAIN_EPOCHS)\n","#     dataset = dataset.batch(batch_size)\n","#     dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n","#     print(\"Pass5\")\n","#     return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6FOscOI2xyF","colab_type":"code","colab":{}},"source":["# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","# # Clean up the model directory if present\n","# shutil.rmtree(MODEL_DIR, ignore_errors=True)\n","# model = build_estimator(MODEL_DIR, LEARN_TYPE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5vKQtysDcjz","colab_type":"code","colab":{}},"source":["\n","# for n in range(TRAIN_EPOCHS // EPOCHS_PER_EVAL):\n","#     model.train(input_fn=lambda: input_fn(\n","#         data_files=train_files,\n","#         num_epochs=EPOCHS_PER_EVAL,\n","#         shuffle=True,\n","#         batch_size=BATCH_SIZE))\n","\n","#     results = model.evaluate(input_fn=lambda: input_fn(\n","#         data_files=test_files,\n","#         num_epochs=1,\n","#         shuffle=False,\n","#         batch_size=BATCH_SIZE))\n","\n","#     # Display evaluation metrics\n","#     print('Results at epoch', (n + 1) * EPOCHS_PER_EVAL)\n","#     print('-' * 60)\n","\n","#     for key in sorted(results):\n","#         print('%s: %s' % (key, results[key]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qsoPPX6qzDW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}