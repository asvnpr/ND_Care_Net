{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recommendation: wide_only.ipynb","provenance":[{"file_id":"1lKjJrVSV5tsz437zS7AuXg_cxDDcUohk","timestamp":1586557870381},{"file_id":"1GmBjHRLF0gIfjSjyT_AmpND4HpUP2yn6","timestamp":1585580865391}],"collapsed_sections":[],"authorship_tag":"ABX9TyMe1oc2+Gr9slcoyUaIoNpx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JTuET-PkyoW9","colab_type":"text"},"source":["## Testing wide deep model example \n","\n","Following [original paper](https://arxiv.org/pdf/1606.07792.pdf) and primarily this [workshop](https://noelkonagai.github.io/Workshops/tensorflow_pt2_widedeep/). Additionally these resources: \n","- [how to build wide deep model w. keras and tf2] (https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b) and its [code](https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb)\n","-[feature crossing tutorial](https://developers.google.com/machine-learning/crash-course/feature-crosses/crossing-one-hot-vectors)\n","\n","Other implementations to look at:  \n","[1](https://github.com/Mohit67/Movie_recommendar/blob/master/wide_n_deep_tutorial.py),[2](https://github.com/wangby511/Recommendation_System), [3](https://github.com/AmoghM/Yelp-Restaurants-RecSys), [4](https://github.com/rajaharsha/Wide-Deep-Neural-Networks), [5](https://github.com/wang-henry4/wide-and-deep-recommender-model),[6](https://devblogs.nvidia.com/accelerating-wide-deep-recommender-inference-on-gpus/), [7](https://humboldt-wi.github.io/blog/research/information_systems_1718/08recommendation/),[8](https://www.youtube.com/watch?v=m_AZrITxs5M&t=0s)"]},{"cell_type":"code","metadata":{"id":"mfZAXRqirJ82","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","prefix = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","!echo \"Project dir contents:\" && ls \"$prefix/\"\n","!echo -e \"\\nColab Notebook home dir:\" && ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxqDBOntzIx1","colab_type":"code","colab":{}},"source":["# !pip uninstall -y dask\n","!pip install -U ipykernel tensorflow-text==2.2.0rc2 plot-keras-history nest_asyncio dask-ml==1.0.0 dask[complete] distributed==1.25.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nB0OorxHziCk","colab_type":"code","colab":{}},"source":["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"Download and clean the Census Income Dataset.\"\"\"\n","\n","import argparse\n","import os\n","import shutil\n","\n","try:\n","    %tensorflow_version 2.x\n","except Exception as e:\n","    print(e)\n","import tensorflow as tf\n","\n","import pandas as pd\n","import json\n","import csv\n","import pickle\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import tensorflow_hub as hub\n","import random\n","import multiprocessing\n","import ast\n","from dask_ml.model_selection import train_test_split\n","from dask_ml.preprocessing import StandardScaler\n","# import tensorflow_text\n","\n","import dask.dataframe as dd\n","from dask.distributed import Client, LocalCluster\n","from dask import delayed\n","from dask import compute\n","\n","cluster = LocalCluster(processes=False)\n","client = Client(cluster)\n","client"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hle4R8av5fxy","colab_type":"code","colab":{}},"source":["try: \n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    # This is the TPU initialization code that has to be at the beginning.\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","except Exception as e:\n","    print(e)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtfxEpCe2DB-","colab_type":"text"},"source":["#### Default Args and names"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7GTNX_xS_YD3","colab":{}},"source":["# tf.enable_eager_execution()\n","# PREFIX = '/afs/crc.nd.edu/user/a/aveganog/ND_Care_Net'\n","PREFIX = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","TRAINING_FILE = 'care-net.train'\n","EVAL_FILE = 'care-net.test'\n","\n","NODE_TYPE = 'services'\n","\n","# needed for file name below\n","MODEL = 'USE'\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n","MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n","MODEL_TYPE = MODEL_URL.split('/')[-2]\n","MODEL_VER = MODEL_URL.split('/')[-1]\n","\n","!mkdir -p \"$prefix/embeddings/$MODEL/\"\n","print(\"Using embeddings from Model {}_{}_v{}\".format(MODEL, MODEL_TYPE, MODEL_VER))\n","\n","DATA_DIR = os.path.join(PREFIX, 'data', 'wide_deep_dataset_chunks')\n","\n","# help = 'Base directory for the model.')\n","# MODEL_TYP = 'wide-deep-USE'\n","MODEL_TYP = 'deep-USE'\n","# MODEL_TYP = 'wide-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', MODEL_TYP)\n","\n","# help = 'Number of training epochs.')\n","TRAIN_EPOCHS = 40\n","\n","# help = 'The number of training epochs to run between evaluations.')\n","EPOCHS_PER_EVAL =2 \n","\n","# help = 'Number of examples per batch.')\n","BATCH_SIZE = 256\n","QBAT_SIZE = 512\n","\n","# parser.add_argument(\n","#     '--train_data', type=str, default='/tmp/census_data/adult.data',\n","#     help='Path to the training data.')\n","\n","# parser.add_argument(\n","#     '--test_data', type=str, default='/tmp/census_data/adult.test',\n","#     help='Path to the test data.')\n","\n","TRAIN_PCNT = 0.7 \n","TEST_PCNT = 1 - TRAIN_PCNT\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Label'\n","]\n","\n","CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0], [''], [0], [0], [0]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3e8OjOnStm6","colab_type":"code","colab":{}},"source":["\"\" # download auxillary data\n","# tf.enable_eager_execution()\n"," # json nodes of 211 IN services\n","with open(PREFIX + '/data/services_nodes.json') as sn:\n","    serv_nodes = json.loads(sn.read())\n","\n","# our heterogeneous information network. \n","# we're only using the services data\n","with open(PREFIX + '/data/HIN_nodes.json') as taxo:\n","    hin_nodes = json.loads(taxo.read())\n","\n","# map service_ids to node numbers in our graph\n","with open(PREFIX + '/data/service_id_to_node_num.json') as sn:\n","    serv_trans = json.loads(sn.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/code_to_node_num.json') as ct:\n","    code_trans = json.loads(ct.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/tagged_texts.json') as ct:\n","    tagged_texts = json.loads(ct.read())\n","\n","# load list of queries/keywords from taxonomy data\n","queries_path = os.path.join(PREFIX, 'data', 'HIN_references.csv')\n","with open(queries_path) as qf:\n","  queries = qf.read()\n","  queries = queries.split(',')\n","  # cleanup some bad data. TODO: Fix in notebook that generates the data\n","  queries = [''] + [q for q in queries if q != '']\n","\n","# with open(os.path.join(PREFIX, 'data', 'service_recommendations.json')) as rf:\n","#     tagged_recs = json.loads(rf.read())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"roKW2lt51vGV","colab_type":"text"},"source":["### Now we can build, run, and eval the test wide deep model"]},{"cell_type":"markdown","metadata":{"id":"3NUaeCiBFOC9","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"sb51fu2p2Fxo","colab_type":"text"},"source":["#### Build input feature columns"]},{"cell_type":"code","metadata":{"id":"TnIMz9DJ14ay","colab_type":"code","colab":{}},"source":["def build_model_columns():\n","    \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n","    # Continuous columns\n","    distance = tf.feature_column.numeric_column('Distance', shape=(1,), dtype=tf.float32)\n","    similarity = tf.feature_column.numeric_column('Embedding_Similarity', shape=(1,), dtype=tf.float32)\n","    # month of referral\n","    month = tf.feature_column.categorical_column_with_identity('Month', num_buckets=13, default_value=0)\n","    month = tf.feature_column.indicator_column(month)\n","\n","    #   prev_recs = tf.feature_column.numeric_column(key='Previously_Recommended',shape=(16547,))\n","    # voc_list = [str(serv_nodes[nid]['serv_id']) for nid in serv_nodes]\n","    # prev_recs = tf.feature_column.categorical_column_with_vocabulary_list(key='Shared_Recommendations', vocabulary_list=voc_list, dtype=tf.string)\n","    # prev_recs = tf.feature_column.embedding_column(prev_recs,32)\n","    serv_id = tf.feature_column.categorical_column_with_identity('Selected_Service_ID', num_buckets=len(serv_nodes))\n","    serv_id = tf.feature_column.indicator_column(serv_id)\n","\n","    cand_id = tf.feature_column.categorical_column_with_identity('Candidate_Service_ID', num_buckets=len(serv_nodes))\n","    cand_id = tf.feature_column.indicator_column(cand_id)\n","    \n","    # bucket_sim = tf.feature_column.bucketized_column(similarity, [0.2,0.4,0.6,0.8])\n","    cross = tf.feature_column.crossed_column(['Selected_Service_ID', 'Candidate_Service_ID'], hash_bucket_size=40000)\n","    # try this later to combine embeddings into one feature:\n","    # https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings\n","    \n","    # Wide columns and deep columns.\n","    base_columns = [\n","        distance, similarity, month\n","    ]\n","\n","    crossed_columns = [\n","        tf.feature_column.embedding_column(cross, 2048)\n","    ]\n","\n","    wide_columns = base_columns + [serv_id, cand_id] + crossed_columns #+ [bucket_sim]\n","\n","    return wide_columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZ_sH_342LzV","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"VTsMoXuC3JT9","colab_type":"code","colab":{}},"source":["data_files = os.listdir(DATA_DIR)\n","data_files = [os.path.join(DATA_DIR, f) for f in data_files]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qq4yQUGquCrQ","colab":{}},"source":["nid_idxs = {}\n","for i, nid in enumerate(hin_nodes):\n","    nid_idxs[nid] = i\n","\n","# save indexes of services to use in our dataset for embedding lookup\n","sid_idxs = {}\n","for i, sid in enumerate(serv_trans):\n","    sid_idxs[sid] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvFKWwpZWdZL","colab_type":"code","colab":{}},"source":["dne_cnt = 0\n","\n","def preproc_query(emb_key):\n","    # idx = None\n","    # if str(emb_key) == 'nan':\n","    #     emb_key = ''\n","    if emb_key in q_idxs:\n","        idx = q_idxs[str(emb_key)]\n","    else:\n","        if str(emb_key) not in tagged_embeds:\n","            tagged_embeds[str(emb_key)] = extract_embed([str(emb_key)])\n","        q_idxs[str(emb_key)] = len(tagged_embeds.keys()) - 1\n","        idx = q_idxs[str(emb_key)]\n","    return idx\n","\n","# def get_recs(nid):\n","#     if str(nid) in serv_nodes:\n","#         sid = hin_nodes[str(nid)]['serv_id']\n","#         return tagged_recs[str(sid).split('.')[0]]\n","#     else:\n","#         global dne_cnt\n","#         dne_cnt += 1\n","#         return [0]*len(serv_nodes.keys())\n","\n","def proc_recs(row):\n","    return shared_recs\n","\n","def preproc_lbl(lbl):\n","    print(int(lbl))\n","    return int(lbl)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGeet-Dw9Yad","colab_type":"code","colab":{}},"source":["CSV_COLUMNS_OG = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Shared_Recommendations',\n","            'Query',\n","            'Label'\n","]\n","\n","DEEP_COLUMNS = [\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Query'\n","]\n","\n","WIDE_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            # 'Shared_Recommendations'\n","            'Selected_Service_ID',\n","            'Candidate_Service_ID'\n","]\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Shared_Recommendations',\n","            'Query',\n","            'Label'\n","]\n","\n","from tqdm import tqdm\n","from sklearn import preprocessing\n","\n","LABEL_NAME = CSV_COLUMNS[-1]\n","serv_ids = None\n","def load_df(files, has_checkpoints=False):\n","\n","    data_df = []\n","    tot = len(files)\n","    for i, f in enumerate(files):\n","        fname = f.split('/')[-1]\n","        # skip hidden files\n","        if fname[0] == '.':\n","            continue\n","        if has_checkpoints:\n","            data_df.append(pd.read_parquet(f))\n","        else:\n","    # None means dask uses a block per file\n","    # data_df = dd.read_csv(os.path.join(DATA_DIR, 'wide*.csv'), blocksize=None)#.head(n=500000)\n","            data_df.append(pd.read_csv(f, nrows=25000))\n","            print(\"loaded file {} {} of {}\".format(fname, i+1, tot))\n","    data_df = pd.concat(data_df)\n","\n","    if not has_checkpoints:\n","        # ignore indexing column\n","        data_df.columns = CSV_COLUMNS_OG\n","        # data_df.compute()\n","        # data_df = data_df.replace(np.nan, '', regex=True)\n","        \n","        data_df = data_df.fillna(-1)\n","        tqdm.pandas()\n","        \n","        serv_ids = data_df[CSV_COLUMNS[3]]#.apply(lambda x: get_sid(x))\n","        cand_ids = data_df[CSV_COLUMNS[4]]#.apply(lambda x: get_sid(x))\n","        labels = data_df.pop('Label')\n","        CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","        CSV_COLUMNS.append('Candidate_Service_ID')\n","        global WIDE_COLUMNS\n","        WIDE_COLUMNS = WIDE_COLUMNS + ['Selected_Service_ID', 'Candidate_Service_ID']\n","        data_df[CSV_COLUMNS[-2]] = np.array(serv_ids.values).astype(np.int32)\n","        data_df[CSV_COLUMNS[-1]] = np.array(cand_ids.values).astype(np.int32)\n","\n","        data_df[CSV_COLUMNS[0]] = data_df[CSV_COLUMNS[0]].fillna(-1.0)#, inplace=True)\n","        data_df[CSV_COLUMNS[1]] = data_df[CSV_COLUMNS[1]].fillna(0.0)#, inplace=True)\n","        data_df[CSV_COLUMNS[2]] = data_df[CSV_COLUMNS[2]].fillna(0)#, inplace=True)\n","        data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].fillna('')#, inplace=True)\n","        data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].fillna('')#, inplace=True)\n","        data_df[CSV_COLUMNS[5]] = data_df[CSV_COLUMNS[5]].fillna('')#, inplace=True)\n","        data_df[CSV_COLUMNS[6]] = data_df[CSV_COLUMNS[6]].fillna('')#, inplace=True)\n","        data_df[CSV_COLUMNS[7]] = data_df[CSV_COLUMNS[7]].fillna(0)#, inplace=True)\n","        data_df[CSV_COLUMNS[8]] = data_df[CSV_COLUMNS[8]].fillna(0)#, inplace=True)\n","\n","        shared_recs = []\n","        # pbar = tqdm(total=500000, mininterval=5, desc='Finding shared recommendations')\n","        # def apply_or(x,y):\n","        #     z = np.logical_or(x,y)\n","        #     return z\n","        # for i in range(500000):\n","        #     z = np.logical_or(serv_id_recs.iloc[i], candidate_id_recs.iloc[i])\n","        #     shared_recs.append(z)\n","        #     pbar.update(1)\n","        \n","        z_score_scaler = StandardScaler()\n","        data_df[CSV_COLUMNS[0]] = z_score_scaler.fit_transform(data_df[CSV_COLUMNS[0]].values.reshape(-1,1))\n","        # data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].progress_apply(lambda f: preproc_serv(f))\n","        # data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].progress_apply(lambda f: preproc_serv(f))\n","        # data_df[CSV_COLUMNS[5]] = list(data_df[CSV_COLUMNS[5]].apply(lambda f: preproc_rec(f)))\n","        # data_df[CSV_COLUMNS[5]] = shared_recs\n","        \n","        # data_df = data_df[CSV_COLUMNS]\n","        \n","        # data_df = data_df[DEEP_COLUMNS]\n","        # data_df = data_df[WIDE_COLUMNS + ['Selected_Service_ID']]\n","        data_df = data_df[WIDE_COLUMNS]\n","        # data_df = data_df.dropna()\n","        # data_df['Shared_Recommendations']\n","    \n","    return train_test_split(data_df.values, labels.values, train_size=0.8, random_state=19, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-unuEDewVX_Z","colab_type":"code","colab":{}},"source":["import glob\n","\n","has_checkpoints = False\n","X_train, X_val, y_train, y_val = load_df(data_files, has_checkpoints)\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=(0.6/0.8), random_state=19, shuffle=True)\n","# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.7/0.9, random_state=19, shuffle=True)\n","print(\"Using train data with shape: {}\".format(X_train.shape))\n","print(\"Using test data with shape: {}\".format(X_test.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiwx55CU-nuD","colab_type":"code","colab":{}},"source":["import dask.array as da\n","dtypes = [np.float32,np.float32,str,str,str,np.array([]),str,str]\n","dtype_ex = pd.Series([0.0,0.0,'','','',[],'',''])\n","\n","X_tr = dd.from_dask_array(da.from_array(X_train, chunks=(2000,5)))\n","X_tr.columns = WIDE_COLUMNS\n","# X_tr['Shared_Recommendations'] = X_tr['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","X_vl = dd.from_dask_array(da.from_array(X_val, chunks=(2000,5)))\n","X_vl.columns = WIDE_COLUMNS\n","# X_vl['Shared_Recommendations'] = X_vl['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","X_te = dd.from_dask_array(da.from_array(X_test, chunks=(2000,5)))\n","X_te.columns = WIDE_COLUMNS\n","# X_te['Shared_Recommendations'] = X_te['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","\n","print(\"Using train dataset with shape: {}\".format(X_tr.shape))\n","print(\"Using validation dataset with shape: {}\".format(X_vl.shape))\n","print(\"Using test dataset with shape: {}\".format(X_te.shape))\n","X_tr.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z5SUTduRVUu","colab_type":"code","colab":{}},"source":["CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Shared_Recommendations',\n","            'Query',\n","            'Selected_Service_ID',\n","            'Candidate_Service_ID'\n","]\n","\n","WIDE_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            # 'Shared_Recommendations',\n","            'Selected_Service_ID',\n","            'Candidate_Service_ID'\n","]\n","\n","# following: https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n","\n","# initialize inputs for model with appropriate keras layers\n","wide_in = {}\n","for i in range(len(CSV_COLUMNS)):\n","    if i in (0,1):\n","        print(\"Wide Input: Added {} input layer as float32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.float32)\n","    # elif i == 5:\n","        # print(\"Wide Input: Added {} input layer as list of strings\".format(CSV_COLUMNS[i]))\n","        # wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,16547), dtype='string')\n","        # inputs[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(16547,), dtype='int32')\n","    elif i in (2,7,8):\n","        print(\"Wide Input: Added {} input layer as int32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.int32)\n","print(wide_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBuENTrDTF57","colab_type":"code","colab":{}},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","\n","wide_columns = build_model_columns()\n","# following: https://blog.tensorflow.org/2018/04/predicting-price-of-wine-with-keras-api-tensorflow.html\n","def make_wide_model(linear_columns, linear_inputs):\n","    # dist = tf.keras.layers.Input(name=CSV_COLUMNS[0], shape=(1,), dtype=tf.float32)\n","    # sim = tf.keras.layers.Input(name=CSV_COLUMNS[1], shape=(1,), dtype=tf.float32)\n","    # month = tf.keras.layers.Input(name=CSV_COLUMNS[2], shape=(1,), dtype=tf.float32)\n","    # recs_in = tf.keras.layers.Input(name=CSV_COLUMNS[5], shape=(16547,), dtype=tf.float32)\n","    # recs = layers.Flatten()(recs_in)\n","    # sid = tf.keras.layers.Input(name=CSV_COLUMNS[-1], shape=(1,), dtype=tf.float32)\n","    # wide_input = layers.concatenate([recs])\n","    wide = layers.DenseFeatures(linear_columns)(linear_inputs)\n","    # wide = layers.Dense(25000)(recs)\n","    pred = layers.Dense(1, activation='sigmoid', name='Pred')(wide)\n","    wide_model = Model(inputs=list(linear_inputs.values()), outputs=pred)\n","    wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(l1_regularization_strength=0.5, l2_regularization_strength=0.5), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","    return wide_model\n","\n","wide_model = make_wide_model(wide_columns, wide_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0pQm7IGWWzV","colab_type":"code","colab":{}},"source":["wide_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkBeTEGMbuDv","colab_type":"code","colab":{}},"source":[" tf.keras.utils.plot_model(wide_model, os.path.join(PREFIX, 'figures', 'deep_keras_model.png'), show_shapes=True, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEDpsJry2t86","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.utils.data_utils import Sequence\n","cnt = 0\n","\n","def dask_gen(X, y):\n","    while True:\n","        for idx in range(len(X_train)):\n","            feats = X[idx].compute()\n","            features = {}\n","            col_types = [np.float32, np.float32, str, None, str]\n","            fs = []\n","            datasets = []\n","            for i, col in enumerate(WIDE_COLUMNS):\n","                col_type = col_types[i]\n","                if col_type is not None:\n","                    features[col] = feats[col].values.astype(col_type)\n","                    fs.append(feats[col].values.astype(col_type))\n","                    ds = tf.data.Dataset.from_tensor_slices(features[col])\n","                # else:\n","                #     print(col)\n","                #     features[col] = feats[col].progress_apply(lambda x: x.to_dense().tolist()).values.reshape(len(feats),1)\n","                #     fs.append(feats[col].progress_apply(lambda x: x.to_dense()).values)\n","                #     f = features[col]\n","                    ds = tf.data.Dataset.from_generator(lambda: f, tf.int32, output_shapes=[None])\n","            # x1,x2,x3,x4,x5 = fs\n","            lbl = np.array(y[idx].compute())\n","            labels = {'Pred': lbl}\n","            # dataset = tf.data.Dataset.zip(tuple(datasets))\n","            global cnt\n","            cnt += 1\n","            # yield x1,x2,x3,x4,x5,lbl\n","            yield features, labels#, dataset\n","\n","# taken from: https://anaconda.org/defusco/keras-dask/notebook\n","class DaskGenerator(Sequence):\n","    def __init__(self, samples, classes):\n","        super().__init__()\n","        '''Initialize a generator of samples and classes for training'''\n","        self.sample_batches = samples.to_delayed()\n","        self.class_batches = classes.to_delayed()\n","        \n","        # assert len(self.sample_batches) == len(self.class_batches), 'lengths of samples and classes do not match'\n","        # assert self.sample_batches.shape[1] == 1, 'all columns should be in each chunk'\n","    \n","    def __len__(self):\n","        '''Total number of batches, equivalent to Dask chunks in 0th dimension'''\n","        return len(self.sample_batches)\n","        \n","    def __getitem__(self, idx):\n","        '''Extract and compute a single batch returned as (X, y)'''\n","        tmp_idx = idx+1 % len(self.sample_batches)\n","        feats = self.sample_batches[idx].compute()\n","        features = {}\n","        # datasets = []\n","        # print(feats)\n","        col_types = [np.float32, np.float32, np.int32, np.int32, np.int32]\n","        for i, col in enumerate(WIDE_COLUMNS):\n","            col_type = col_types[i]\n","            features[col] = feats[col].values.astype(col_type)\n","            ds = tf.data.Dataset.from_tensor_slices(features[col])\n","            \n","        lbl = np.array(self.class_batches[idx].compute())\n","        labels = {'Pred': lbl}\n","        global cnt\n","        cnt += 1\n","        return features, labels\n","        # return tf.convert_to_tensor(list(self.sample_batches[idx].compute())),  tf.convert_to_tensor(self.class_batches[idx].compute())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXgGv5muqaA7","colab_type":"code","colab":{}},"source":["X_parts = X_tr.partitions\n","y_parts = dd.from_array(y_train).partitions\n","data_iter = dask_gen(X_parts,y_parts)\n","# features, labels = next(data_iter)\n","# with strategy.scope():\n","    # needs to be created inside here for TPU to work\n","    # wide_model = make_wide_model(wide_columns, wide_in)\n","    # train_ds = wide_input_fn(X_tr, y_train)\n","    # wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","# train_gen = DaskGenerator(X_tr,dd.from_array(y_train))\n","\n","feed_dict = {}\n","val_dict = {}\n","for col in WIDE_COLUMNS:\n","    feed_dict[col] = X_tr[col].compute()\n","    val_dict[col] = X_vl[col].compute()\n","labels = {'Pred': y_train}\n","dataset = tf.data.Dataset.from_tensor_slices((feed_dict, labels))\n","dataset = dataset.shuffle(buffer_size=len(X_train))\n","dataset = dataset.repeat(TRAIN_EPOCHS)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","model_hist = wide_model.fit(dataset, verbose=1,use_multiprocessing=False, validation_data=(list(val_dict.values()), y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj8EgobRELIl","colab_type":"code","colab":{}},"source":["from plot_keras_history import plot_history\n","import matplotlib.pyplot as plt\n","\n","# plot keras history metrics \n","plot_history(model_hist.history)\n","plt.show()\n","plot_history(model_hist.history, path=\"standard.png\")\n","plt.close()\n","\n","# follow for full model report\n","# https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n","# https://www.machinecurve.com/index.php/2019/10/08/how-to-visualize-the-training-process-in-keras/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfpVF55bcokI","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_generator(lambda: dask_gen(X_tr,dd.from_array(y_train)), output_types=[tf.float32, tf.float32, tf.string, None,tf.string])\n","# dataset = dataset.map(parse_line, num_parallel_calls=4)\n","if shuffle:\n","    dataset = dataset.shuffle(buffer_size=len(X))\n","    dataset = dataset.repeat(TRAIN_EPOCHS)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8zSC4fPVuMG","colab_type":"code","colab":{}},"source":["MODEL_DIR = os.path.join(PREFIX, 'models', MODEL_TYP, \"init_baseline_deep-only\")#.format(BATCH_SIZE)) #+ str(random.randint(1,1001))\n","print(\"Using model dir {}\".format(MODEL_DIR))\n","\n","config = tf.estimator.RunConfig(model_dir=MODEL_DIR, tf_random_seed=42, save_summary_steps=100,\n","    save_checkpoints_steps=250, session_config=None, keep_checkpoint_max=50, \n","    keep_checkpoint_every_n_hours=1,session_creation_timeout_secs=7200)\n","\n","def rec_line_gen(R):\n","    for r in range(len(R)):\n","        print(R.shape)\n","        print(r)\n","        yield np.asarray(R[r])\n","\n","def wide_input_generator(X,y):\n","    while True:\n","        X = np.array(X)\n","        for i in range(len(X)):\n","            # WIDE: \n","            x1,x2,x3,x4 = X[:,i]\n","            yield np.float32(x1),np.float32(x2),str(x3),str(x4),y[i]\n","\n","def wide_input_fn(X, y, shuffle=True):\n","\n","    # # WIDE ONLY\n","    def parse_line(f1,f2,f3,f4,f5):\n","        cols = [f1,f2,f3,f4,f5]\n","        lbl = cols.pop(-1)\n","        labels = {'Pred': lbl}\n","        feats = dict(zip(WIDE_COLUMNS + ['Selected_Service_ID'], cols))\n","        return feats,labels\n","\n","    dist = X['Distance'].compute().values.astype(np.float32)\n","    sim = X['Embedding_Similarity'].compute().values.astype(np.float32)\n","    mon = X['Month'].apply(lambda x: str(x).split('.')[0], meta=('Month', 'object')).copmute().values.astype('U1')\n","    recs = X['Shared_Recommendations']\n","    recs = []\n","\n","    def sparse_to_dense(v, recs):\n","        recs.append(delayed(pandas.SparseArray.to_dense, pure=False, name='sparseToDenseRecs')(v))\n","        return v\n","    X['Shared_Recommendations'].apply(lambda v: sparse_to_dense(v, recs), meta=('sparseV', 'object'))\n","    X['Shared_Recommendations'] = da.asarray(compute(*recs))\n","    \n","    sid = X['Selected_Service_ID'].apply(lambda x: str(x).split('.')[0], meta=('serv_ID', 'object')).values.astype('U6')\n","\n","    X.apply(lambda x: x.compute(), axis=1, meta=('Col', 'object'))\n","\n","    ds1 = tf.data.Dataset.from_tensors(dist)\n","    ds2 = tf.data.Dataset.from_tensor_slices(sim)\n","    ds3 = tf.data.Dataset.from_tensor_slices(mon)\n","    ds6 = tf.data.Dataset.from_tensors(recs)\n","    ds8 = tf.data.Dataset.from_tensor_slices(sid)\n","    ds9 = tf.data.Dataset.from_tensor_slices(y)\n","    \n","    ds = tf.data.Dataset.from_generator()\n","    dataset = tf.data.Dataset.zip((ds1,ds2,ds3,ds6,ds8,ds9))\n","    dataset = dataset.map(parse_line, num_parallel_calls=4)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(X))\n","        dataset = dataset.repeat(TRAIN_EPOCHS)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    # dataset = dataset.cache() \n","    return dataset\n","\n","ds = wide_input_fn(X_tr, y_train)\n","X_tr.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpMUnZ-VaqwP","colab_type":"code","colab":{}},"source":["train_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    if col in ('Month', 'Selected_Service_ID'):\n","        train_dict[col] = X_tr[col].values.astype('U6')\n","        print(train_dict[col].dtype)\n","    else:\n","        train_dict[col] = X_tr[col].values\n","\n","val_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    if col in ('Month', 'Selected_Service_ID'):\n","        val_dict[col] = X_vl[col].values.astype('U6')\n","        # print(val_dict[col].dtype)\n","    else:\n","        val_dict[col] = X_vl[col].values\n","\n","# with strategy.scope():\n","    # needs to be created inside here for TPU to work\n","wide_model = make_wide_model(wide_columns, wide_in)\n","    # train_ds = wide_input_fn(X_tr, y_train)\n","wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","train_gen = DaskGenerator(X_tr,dd.from_array(y_train))\n","os.environ['TF_KERAS'] = '1'\n","\n","# model_hist = wide_model.fit(train_gen, verbose=1,use_multiprocessing)#, validation_data=(list(val_dict.values()), y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S8Av5426WvHA","colab_type":"code","colab":{}},"source":["print(cnt)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1cWazdK3kK1","colab_type":"code","colab":{}},"source":["model_estimator = tf.keras.estimator.model_to_estimator(wide_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1j8xmV0bnIE","colab_type":"code","colab":{}},"source":["feed_dict = {}\n","for col in WIDE_COLUMNS + ['Selected_Service_ID']:\n","    feed_dict[col] = X_te[col].values\n","# feed_dict.update({'Pred': y_train})\n","wide_model.evaluate(x=list(feed_dict.values()), y=y_test, batch_size=BATCH_SIZE, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9VnsMLYdQ7l","colab_type":"code","colab":{}},"source":["MODEL_TYP = 'wide-deep-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', 'wide_and_deep_{}'.format(MODEL), MODEL_TYP)\n","model_fpath = os.path.join(MODEL_DIR, '{}-model.h5'.format(MODEL_TYP)\n","deep_model.save(model_fpath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hU0xWRDVqbW","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir \"$MODEL_DIR/\" #--debugger_port 6969 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8l-RCB_0aW2I","colab_type":"code","colab":{}},"source":["train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(X_te, y_test), name='wide-deep-care-net_batch-size_{}'.format(BATCH_SIZE),\n","                                  start_delay_secs=2, throttle_secs=10)\n","evals, exports = tf.estimator.train_and_evaluate(model_estimator, train_spec, eval_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MIFUYocrONQ","colab_type":"code","colab":{}},"source":["train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(X_te, y_test), name='wide-deep-care-net_batch-size_{}'.format(BATCH_SIZE),\n","                                  start_delay_secs=2, throttle_secs=10)\n","evals, exports = tf.estimator.train_and_evaluate(wide_deep, train_spec, eval_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsvjtg-FtKcK","colab_type":"code","colab":{}},"source":["X_tr['Selected_Service_Embedding'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zI5maBHdyBbv","colab_type":"code","colab":{}},"source":["import math\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","# train_ds = input_fn(X_tr, y_train)\n","# test_ds = input_fn(X_te, y_test, shuffle=False)\n","num_5k_steps = math.ceil(len(X_te) / 5000) \n","train_steps = math.ceil(len(X_tr) / num_5k_steps)\n","\n","accs, precs, recs, f1s = [],[],[],[]\n","for i in range(num_5k_steps):\n","    print('='*20, \"Train/Eval round {}/{}\".format(i,num_5k_steps), '='*20)\n","    eval_er.train(input_fn=lambda: input_fn(X_tr, y_train), steps=train_steps)\n","\n","    start_idx = i * train_steps\n","    end_idx = start_idx + train_steps\n","    print(\"Using train samples {} to {}\".format(start_idx,end_idx))\n","    X_metr = None\n","    preds = None\n","    y_metr = None\n","    if end_idx <= len(X_train):\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx:end_idx]\n","        print(\"Using subset of train data with size {}\".format(len(X_metr)))\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx:end_idx]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    else:\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx::]\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx::]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    yhat = []\n","    for i in range(train_steps):\n","        pred = next(preds)\n","        print(pred.keys())\n","        yhat.append(pred['Pred'])\n","    print(\"Going through {} predictions for training subset\".format(len(yhat)))\n","    yhat_classes = list(map(lambda x: np.round(x),yhat))\n","    # accuracy: (tp + tn) / (p + n)\n","    accuracy = accuracy_score(y_metr, yhat_classes)\n","    print('Accuracy: %f' % accuracy)\n","    # precision tp / (tp + fp)\n","    precision = precision_score(y_metr, yhat_classes)\n","    print('Precision: %f' % precision)\n","    # recall: tp / (tp + fn)\n","    recall = recall_score(y_metr, yhat_classes)\n","    print('Recall: %f' % recall)\n","    # f1: 2 tp / (2 tp + fp + fn)\n","    f1 = f1_score(y_metr, yhat_classes)\n","    print('F1 score: %f' % f1)\n","    accs.append(accuracy)\n","    precs.append(precision)\n","    recs.append(recall)\n","    f1s.append(f1)\n","\n","    eval_er.evaluate(input_fn=lambda: input_fn(X_te, y_test, shuffle=False), steps=5000, name='5kEvals')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GR-lfvR_1up","colab_type":"code","colab":{}},"source":["    cfname = os.path.join(MODEL_DIR, 'label_metadata.tsv')\n","with open(fname, 'w') as f:\n","    f.write(\"Index\\tLabel\\tRecommended\\n\")\n","    sids = list(tagged_embeds.keys())\n","    for idx in range(len(sids)):\n","        typ = None\n","        if sids[idx] in hin_nodes:\n","            name = hin_nodes[sids[idx]]['name']\n","            typ = 'Service'\n","        else:\n","            name = sids[idx]\n","            typ = 'Query'\n","        # sel_name = serv_nodes[sel_idx]['name']\n","        # cand_idx = str(int(X_t['Candidate_Service_Embedding'].values[idx]))\n","        # try:\n","            # cand_name = serv_nodes[cand_idx]['name']\n","        # except:\n","            # continue\n","        # name = \"{}-{}\".format(sel_name, cand_name)\n","        f.write(\"{}\\t{}\\t{}\\n\".format(idx, name, typ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLTXPnxjaK-B","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# yhat = np.array([tf.argmax(pred['Pred'],1) for pred in preds])[:,0]\n","yhat = []\n","for pred in preds:\n","    yhat.append(pred['Pred'])\n","yhat_classes = list(map(lambda x: np.round(x),yhat))\n","\n","\n","# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(y_test, yhat_classes)\n","print('Accuracy: %f' % accuracy)\n","# precision tp / (tp + fp)\n","precision = precision_score(y_test, yhat_classes)\n","print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_test, yhat_classes)\n","print('Recall: %f' % recall)\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(y_test, yhat_classes)\n","print('F1 score: %f' % f1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"odCFG6HKQ-zx","colab_type":"code","colab":{}},"source":["import datetime\n","X_t = pd.DataFrame(X_test)\n","X_t.columns = CSV_COLUMNS\n","# X_t.columns = DEEP_COLUMNS\n","# X_t.columns = WIDE_COLUMNS #+ ['Selected_Service_ID']\n","\n","\n","preds = eval_er.evaluate(lambda: input_fn(X_t, y_test,shuffle=False), name='evalShuffled')\n","# preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# print(next(preds))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hh6r6hDtdEFv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVyEaLfOrnNp","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"AlBSM3DG_ClO","colab_type":"code","colab":{}},"source":["    !rm -rf MODEL_DIR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"reOkfht5yzyY","colab_type":"code","colab":{}},"source":["print(\"Loading model as keras model from {}\".format(export_path))\n","export_path = os.path.join('/tmp/census_model', \"1585107419\")\n","keras_model = tf.keras.models.load_model(export_path, compile=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2YQj0gN3tA0","colab_type":"code","colab":{}},"source":["print(keras_model.asset_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l82RDk-ApgrZ","colab_type":"code","colab":{}},"source":["keras_model.tensorflow_version\n","tf.keras.utils.plot_model(keras_model, 'census_test_model.png', show_shapes=False, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4vTaKAyVLqu","colab_type":"code","colab":{}},"source":["!ls \"$export_path/../\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok6gHp-Gzwoo","colab_type":"code","colab":{}},"source":["# FLAGS, unparsed = parser.parse_known_args()\n","# tf.app.run(argv=[sys.argv[0]] + unparsed)\n","\n","# if not tf.compat.v1.gfile.Exists(FLAGS.data_dir):\n","#     tf.compat.v1.gfile.MkDir(FLAGS.data_dir)\n","\n","# data_df = pd.DataFrame()\n","# data_df, labels = load_df(DATA_DIR)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2wvMcd5b7xY","colab_type":"code","colab":{}},"source":["# dataset = complete_dataset(data_df)\n","# TEST_PCT = 0.3\n","# test_size = int(TEST_PCT * len(dataset))\n","# train_size = int(len(dataset) - test_size)\n","\n","# test_data = dataset.take(test_size)\n","# train_data = dataset.skip(test_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cxGgku62SPs","colab_type":"code","colab":{}},"source":["# def build_estimator(model_dir, model_type):\n","#   \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n","#   wide_columns, deep_columns = build_model_columns()\n","#   hidden_units = [100, 75, 50, 25]\n","\n","#   # Create a tf.estimator.RunConfig to ensure the model is run on GPU\n","#   run_config = tf.estimator.RunConfig().replace(\n","#       session_config=tf.compat.v1.ConfigProto(device_count={'GPU': 1}))\n","\n","#   if LEARN_TYPE == 'wide':\n","#     return tf.estimator.LinearClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=wide_columns,\n","#         config=run_config)\n","#   elif LEARN_TYPE == 'deep':\n","#     return tf.estimator.DNNClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=deep_columns,\n","#         hidden_units=hidden_units,\n","#         config=run_config)\n","#   else:\n","#     return tf.estimator.DNNLinearCombinedClassifier(\n","#         model_dir=model_dir,\n","#         linear_feature_columns=wide_columns,\n","#         dnn_feature_columns=deep_columns,\n","#         dnn_hidden_units=hidden_units,\n","#         config=run_config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eI4sWLGg2cxg","colab_type":"code","colab":{}},"source":["def input_fn(data_files, num_epochs, shuffle, batch_size):\n","#     \"\"\"Generate an input function for the Estimator.\"\"\"\n","#     # assert tf.compat.v1.gfile.Exists(data_file), (\n","#     #     '%s not found. Please make sure you have either run data_download.py or '\n","#     #     'set both arguments --train_data and --test_data.' % data_file)\n","\n","#     def parse_dataset_lines(value):\n","#         # print('Parsing dataset...')\n","#         columns = tf.io.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS, select_cols=[0,1,2,3,4,6,7])\n","#         print(columns)\n","#         print()\n","#         # refactor our data. TODO change this directly when creating dataset\n","#         lbl = columns.pop(-1)\n","#         labels = {'Label': tf.py_function(func=preproc_lbl,\n","#                                                 inp=[lbl],\n","#                                                 Tout=tf.int32)}\n","#         # labels = lbl\n","#         CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","#         columns = columns + [columns[3]]\n","#         print(\"Pass1\")\n","#         features = dict(zip(CSV_COLUMNS, columns))\n","#         features.pop(CSV_COLUMNS[5])\n","#         features[CSV_COLUMNS[-1]] = columns[3]\n","#         print(\"Pass2\")\n","#         for i in (3,4):\n","#             feat = features[CSV_COLUMNS[i]]\n","#             features[CSV_COLUMNS[i]] = tf.py_function(func=preproc_serv,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_query,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         print(features)\n","#         # features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_rec,\n","#                                                             #  inp=[features[CSV_COLUMNS[5]]],\n","#         print(\"Pass3\")                                        #  Tout=(tf.int32))\n","#         return features, labels\n","        \n","#     # Extract lines from input files using the Dataset API.\n","#     # data_files = tf.io.matching_files(tf.convert_to_tensor(data_files))\n","    \n","#     dfs = tf.data.Dataset.from_tensor_slices(data_files)\n","#     # dataset = tf.data.TextLineDataset(data_files)\n","#     dataset = tf.data.TextLineDataset(data_files)\n","#     print(dataset)\n","#     if shuffle:\n","#         dataset = dataset.shuffle(buffer_size=TRAIN_SIZE)\n","#     dataset = dataset.map(parse_dataset_lines, num_parallel_calls=multiprocessing.cpu_count())\n","#     print(\"Pass4\")\n","#     # dataset = dataset.map(fn, num_parallel_calls=multiprocessing.cpu_count())\n","#     # dataset = dataset.map(parse_dataset, num_parallel_calls=multiprocessing.cpu_count())\n","\n","#     # We call repeat after shuffling, rather than before, to prevent separate\n","#     # epochs from blending together.\n","#     print(dataset)\n","#     dataset = dataset.repeat(TRAIN_EPOCHS)\n","#     dataset = dataset.batch(batch_size)\n","#     dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n","#     print(\"Pass5\")\n","#     return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6FOscOI2xyF","colab_type":"code","colab":{}},"source":["# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","# # Clean up the model directory if present\n","# shutil.rmtree(MODEL_DIR, ignore_errors=True)\n","# model = build_estimator(MODEL_DIR, LEARN_TYPE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5vKQtysDcjz","colab_type":"code","colab":{}},"source":["\n","# for n in range(TRAIN_EPOCHS // EPOCHS_PER_EVAL):\n","#     model.train(input_fn=lambda: input_fn(\n","#         data_files=train_files,\n","#         num_epochs=EPOCHS_PER_EVAL,\n","#         shuffle=True,\n","#         batch_size=BATCH_SIZE))\n","\n","#     results = model.evaluate(input_fn=lambda: input_fn(\n","#         data_files=test_files,\n","#         num_epochs=1,\n","#         shuffle=False,\n","#         batch_size=BATCH_SIZE))\n","\n","#     # Display evaluation metrics\n","#     print('Results at epoch', (n + 1) * EPOCHS_PER_EVAL)\n","#     print('-' * 60)\n","\n","#     for key in sorted(results):\n","#         print('%s: %s' % (key, results[key]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qsoPPX6qzDW","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}