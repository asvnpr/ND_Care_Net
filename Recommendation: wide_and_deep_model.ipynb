{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recommendation: wide_and_deep_model.ipynb","provenance":[{"file_id":"1lKjJrVSV5tsz437zS7AuXg_cxDDcUohk","timestamp":1586558771063},{"file_id":"1GmBjHRLF0gIfjSjyT_AmpND4HpUP2yn6","timestamp":1585580865391}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMjZlrQIfSaM3TtPAHYlpZj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JTuET-PkyoW9","colab_type":"text"},"source":["## Testing wide deep model example \n","\n","Following [original paper](https://arxiv.org/pdf/1606.07792.pdf) and primarily this [workshop](https://noelkonagai.github.io/Workshops/tensorflow_pt2_widedeep/). Additionally these resources: \n","- [how to build wide deep model w. keras and tf2] (https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b) and its [code](https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb)\n","-[feature crossing tutorial](https://developers.google.com/machine-learning/crash-course/feature-crosses/crossing-one-hot-vectors)\n","\n","Other implementations to look at:  \n","[1](https://github.com/Mohit67/Movie_recommendar/blob/master/wide_n_deep_tutorial.py),[2](https://github.com/wangby511/Recommendation_System), [3](https://github.com/AmoghM/Yelp-Restaurants-RecSys), [4](https://github.com/rajaharsha/Wide-Deep-Neural-Networks), [5](https://github.com/wang-henry4/wide-and-deep-recommender-model),[6](https://devblogs.nvidia.com/accelerating-wide-deep-recommender-inference-on-gpus/), [7](https://humboldt-wi.github.io/blog/research/information_systems_1718/08recommendation/),[8](https://www.youtube.com/watch?v=m_AZrITxs5M&t=0s)"]},{"cell_type":"code","metadata":{"id":"mfZAXRqirJ82","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","prefix = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","!echo \"Project dir contents:\" && ls \"$prefix/\"\n","!echo -e \"\\nColab Notebook home dir:\" && ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxqDBOntzIx1","colab_type":"code","colab":{}},"source":["!pip install -U ipykernel tensorflow-text==2.2.0rc2 plot-keras-history nest_asyncio dask-ml==1.0.0 dask[complete] distributed==1.25.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nB0OorxHziCk","colab_type":"code","colab":{}},"source":["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"Download and clean the Census Income Dataset.\"\"\"\n","\n","import argparse\n","import os\n","import shutil\n","\n","try:\n","    %tensorflow_version 2.x\n","except Exception as e:\n","    print(e)\n","import tensorflow as tf\n","\n","import pandas as pd\n","import json\n","import csv\n","import pickle\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import tensorflow_hub as hub\n","import random\n","import multiprocessing\n","# from sklearn.model_selection import train_test_split\n","# from dask_ml.model_selection import train_test_split\n","# from dask_ml.preprocessing import StandardScaler\n","# import tensorflow_text\n","\n","# import dask.dataframe as dd\n","# from dask.distributed import Client, LocalCluster\n","# from dask import delayed\n","# from dask import compute\n","\n","# cluster = LocalCluster(processes=False)\n","# client = Client(cluster)\n","# client\n","# import tensorflow_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtfxEpCe2DB-","colab_type":"text"},"source":["#### Default Args and names"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7GTNX_xS_YD3","colab":{}},"source":["# tf.enable_eager_execution()\n","# PREFIX = '/afs/crc.nd.edu/user/a/aveganog/ND_Care_Net'\n","PREFIX = './drive/My Drive/ND_CSE/Year_1/Research:Care-Net/JBDF_CareNet/Care-Net Backend/code_and_data'\n","TRAINING_FILE = 'care-net.train'\n","EVAL_FILE = 'care-net.test'\n","\n","NODE_TYPE = 'services'\n","\n","\n","MODEL = 'USE'\n","\n","# version of model to use from tensorflow hub\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'\n","# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n","MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n","MODEL_TYPE = MODEL_URL.split('/')[-2]\n","MODEL_VER = MODEL_URL.split('/')[-1]\n","\n","!mkdir -p \"$prefix/embeddings/$MODEL/\"\n","print(\"Using embeddings from Model {}_{}_v{}\".format(MODEL, MODEL_TYPE, MODEL_VER))\n","\n","DATA_DIR = os.path.join(PREFIX, 'data', 'wide_deep_dataset_chunks')\n","\n","# help = 'Base directory for the model.')\n","MODEL_TYP = 'wide-deep-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', 'wide_and_deep_{}'.format(MODEL), MODEL_TYP)\n","\n","# help = 'Number of training epochs.')\n","TRAIN_EPOCHS = 15\n","\n","# 'The number of training epochs to run between evaluations.')\n","# for older version that used TF estimator\n","# EPOCHS_PER_EVAL =2 \n","\n","# 'Number of examples per batch.')\n","BATCH_SIZE = 256\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Label'\n","]\n","\n","CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0], [''], [0], [0], [0]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3e8OjOnStm6","colab_type":"code","colab":{}},"source":[" # download auxillary data\n","\n"," # json nodes of 211 IN services\n","with open(PREFIX + '/data/services_nodes.json') as sn:\n","    serv_nodes = json.loads(sn.read())\n","\n","# our heterogeneous information network. \n","# we're only using the services data\n","with open(PREFIX + '/data/HIN_nodes.json') as taxo:\n","    hin_nodes = json.loads(taxo.read())\n","\n","# map service_ids to node numbers in our graph\n","with open(PREFIX + '/data/service_id_to_node_num.json') as sn:\n","    serv_trans = json.loads(sn.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/code_to_node_num.json') as ct:\n","    code_trans = json.loads(ct.read())\n","\n","# map 211 taxonomy codes to their node number in our graph\n","with open(PREFIX + '/data/tagged_texts.json') as ct:\n","    tagged_texts = json.loads(ct.read())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"roKW2lt51vGV","colab_type":"text"},"source":["### Now we can build, run, and eval the test wide deep model"]},{"cell_type":"markdown","metadata":{"id":"3NUaeCiBFOC9","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"sb51fu2p2Fxo","colab_type":"text"},"source":["#### Build input feature columns"]},{"cell_type":"code","metadata":{"id":"TnIMz9DJ14ay","colab_type":"code","colab":{}},"source":["def build_model_columns():\n","    \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n","    # Continuous columns\n","    distance = tf.feature_column.numeric_column('Distance', shape=(1,), dtype=tf.float32)\n","    similarity = tf.feature_column.numeric_column('Embedding_Similarity', shape=(1,))\n","    month = tf.feature_column.categorical_column_with_hash_bucket('Month', hash_bucket_size=13)\n","    month = tf.feature_column.indicator_column(month)\n","\n","    selected_emb = hub.text_embedding_column('Selected_Service_Embedding', 'https://tfhub.dev/google/universal-sentence-encoder/2')\n","    candidate_emb = hub.text_embedding_column('Candidate_Service_Embedding', 'https://tfhub.dev/google/universal-sentence-encoder/2')\n","    query_emb = hub.text_embedding_column('Query', 'https://tfhub.dev/google/universal-sentence-encoder/2') \n","\n","    #   prev_recs = tf.feature_column.numeric_column(key='Previously_Recommended',shape=(16547,))\n","    # voc_list = [str(serv_nodes[nid]['serv_id']) for nid in serv_nodes]\n","    # prev_recs = tf.feature_column.categorical_column_with_vocabulary_list(key='Shared_Recommendations', vocabulary_list=voc_list, dtype=tf.string)\n","    # prev_recs = tf.feature_column.embedding_column(prev_recs,32)\n","    serv_id = tf.feature_column.categorical_column_with_hash_bucket('Selected_Service_ID', hash_bucket_size=len(serv_nodes))\n","    serv_id = tf.feature_column.embedding_column(serv_id, 32)\n","    #cand_id = tf.feature_column.categorical_column_with_hash_bucket('Candidate_Service_ID', hash_bucket_size=len(serv_nodes))\n","    #cand_id = tf.feature_column.embedding_column(cand_id, 32)\n","    \n","    # bucket_sim = tf.feature_column.bucketized_column(similarity, [0.2,0.4,0.6,0.8])\n","    #cross = tf.feature_column.crossed_column(['Selected_Service_ID', 'Candidate_Service_ID'], hash_bucket_size=25000)\n","    # try this later to combine embeddings into one feature:\n","    # https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings\n","    \n","    # Wide columns and deep columns.\n","    base_columns = [\n","        distance, similarity, month#, prev_recs\n","    ]\n","\n","    #crossed_columns = [\n","    #    tf.feature_column.embedding_column(cross, 512)\n","    #]\n","\n","    wide_columns = base_columns + [serv_id]#, cand_id] #+ crossed_columns #+ [bucket_sim]\n","\n","    deep_columns = [\n","        selected_emb, candidate_emb, query_emb\n","    ]\n","\n","    return wide_columns, deep_columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZ_sH_342LzV","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"VTsMoXuC3JT9","colab_type":"code","colab":{}},"source":["data_files = os.listdir(DATA_DIR)\n","data_files = [os.path.join(DATA_DIR, f) for f in data_files]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qq4yQUGquCrQ","colab":{}},"source":["nid_idxs = {}\n","for i, nid in enumerate(hin_nodes):\n","    nid_idxs[nid] = i\n","\n","# save indexes of services to use in our dataset for embedding lookup\n","sid_idxs = {}\n","for i, sid in enumerate(serv_trans):\n","    sid_idxs[sid] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvFKWwpZWdZL","colab_type":"code","colab":{}},"source":["max_rec_len = 0\n","\n","def preproc_query(emb_key):\n","    # idx = None\n","    # if str(emb_key) == 'nan':\n","    #     emb_key = ''\n","    if emb_key in q_idxs:\n","        idx = q_idxs[str(emb_key)]\n","    else:\n","        if str(emb_key) not in tagged_embeds:\n","            tagged_embeds[str(emb_key)] = extract_embed([str(emb_key)])\n","        q_idxs[str(emb_key)] = len(tagged_embeds.keys()) - 1\n","        idx = q_idxs[str(emb_key)]\n","    return idx\n","\n","# return the full text associated with a service\n","def preproc_serv(node_num):\n","    text = ''\n","    if str(node_num) == 'nan':\n","        node_num = ''\n","    if str(node_num) in sid_idxs:\n","        nid = serv_trans[str(node_num)]\n","        text = ' '.join(tagged_texts[nid])\n","    elif str(node_num) in nid_idxs:\n","        nid = str(node_num)\n","        text = ' '.join(tagged_texts[nid])\n","    # elif serv_trans[str(node_num)] in sid_idxs:\n","        # idx = sid_idxs[str(node_num)]\n","        # text = '!'\n","    return text\n","\n","def preproc_rec(rec):\n","    serv_ids = [sid for sid in list(serv_trans.keys())]\n","    rec = rec.replace('\\'', '\"')\n","    # print(type(rec))\n","    recs = json.loads(rec)\n","    global max_rec_len\n","    len_recs = len(recs)\n","    if len_recs > max_rec_len:\n","        max_rec_len = len_recs\n","        \n","    return recs\n","\n","def pad_recs(recs):\n","    global max_rec_len\n","    for i in range((max_rec_len - len(recs))):\n","        recs.append('')\n","    return recs\n","\n","def preproc_lbl(lbl):\n","    print(int(lbl))\n","    return int(lbl)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGeet-Dw9Yad","colab_type":"code","colab":{}},"source":["CSV_COLUMNS_SEL = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Query',\n","            'Selected_Service_ID',\n","#            'Candidate_Service_ID'\n","]\n","\n","CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Label'\n","]\n","\n","from tqdm import tqdm\n","from sklearn import preprocessing\n","\n","LABEL_NAME = CSV_COLUMNS[-1]\n","serv_ids = None\n","def load_df(files):\n","\n","    data_df = []\n","    tot = len(files)\n","    for i, f in enumerate(files):\n","        fname = f.split('/')[-1]\n","        # skip hidden files\n","        if fname[0] == '.':\n","            continue\n","        if has_checkpoints:\n","            data_df.append(pd.read_parquet(f))\n","        else:\n","            data_df.append(pd.read_csv(f, nrows=7500))\n","        print(\"loaded file {} {} of {}\".format(fname, i+1, tot))\n","    data_df = pd.concat(data_df)\n","\n","    \n","    # ignore indexing column\n","    data_df.columns = CSV_COLUMNS\n","    data_df = data_df.fillna(-1)\n","    tqdm.pandas()\n","    \n","    serv_ids = data_df[CSV_COLUMNS[3]]#.apply(lambda x: get_sid(x))\n","    serv_ids = serv_ids.apply(lambda x: hin_nodes[str(x)]['serv_id'])\n","#       cand_ids = data_df[CSV_COLUMNS[4]]#.apply(lambda x: get_sid(x))\n","#       cand_ids = cand_ids.apply(lambda x: hin_nodes[str(x)]['serv_id'])\n","    # labels = data_df.pop('Label')\n","    CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","#       CSV_COLUMNS.append('Candidate_Service_ID')\n","    # global WIDE_COLUMNS\n","    # WIDE_COLUMNS = WIDE_COLUMNS + ['Selected_Service_ID', 'Candidate_Service_ID']\n","    data_df[CSV_COLUMNS[-1]] = np.array(serv_ids.values).astype(np.int32)\n","    #data_df[CSV_COLUMNS[-1]] = np.array(cand_ids.values).astype(np.int32)\n","\n","    data_df[CSV_COLUMNS[0]] = data_df[CSV_COLUMNS[0]].fillna(-1.0)#, inplace=True)\n","    data_df[CSV_COLUMNS[1]] = data_df[CSV_COLUMNS[1]].fillna(0.0)#, inplace=True)\n","    data_df[CSV_COLUMNS[2]] = data_df[CSV_COLUMNS[2]].fillna(0)#, inplace=True)\n","    data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].fillna('')#, inplace=True)\n","    data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].fillna('')#, inplace=True)\n","    data_df[CSV_COLUMNS[5]] = data_df[CSV_COLUMNS[5]].fillna('')#, inplace=True)\n","    data_df[CSV_COLUMNS[6]] = data_df[CSV_COLUMNS[6]].fillna('')#, inplace=True)\n","    # data_df[CSV_COLUMNS[7]] = data_df[CSV_COLUMNS[7]].fillna(0)#, inplace=True)\n","    # data_df[CSV_COLUMNS[8]] = data_df[CSV_COLUMNS[8]].fillna(0)#, inplace=True)\n","    # data_df.replace({CSV_COLUMNS[7]:{np.nan: 0}})\n","    \n","    \n","    \n","    # data_df = data_df.dropna()\n","    tqdm.pandas()\n","    \n","    # normalize our Distance feature\n","    z_score_scaler = StandardScaler()\n","    data_df[CSV_COLUMNS[0]] = z_score_scaler.fit_transform(data_df[CSV_COLUMNS[0]].values.reshape(-1,1))\n","    data_df[CSV_COLUMNS[3]] = data_df[CSV_COLUMNS[3]].progress_apply(lambda f: preproc_serv(f))\n","    data_df[CSV_COLUMNS[4]] = data_df[CSV_COLUMNS[4]].progress_apply(lambda f: preproc_serv(f))\n","    # data_df[CSV_COLUMNS[5]] = list(data_df[CSV_COLUMNS[5]].apply(lambda f: preproc_rec(f)))\n","    # data_df[CSV_COLUMNS[5]] = list(data_df[CSV_COLUMNS[5]].apply(lambda f: pad_recs(f)))\n","    # data_df = data_df[CSV_COLUMNS_SEL]\n","    \n","    #return train_test_split(data_df.values, labels.values, train_size=0.8, random_state=19, shuffle=True)\n","    # split by grouping services according to Selected_Service_ID. ie make sure each selected service only appears in a set once\n","    train_idxs, val_idxs = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(data_df, groups=data_df['Selected_Service_ID']))    \n","    train_df, val_df = data_df.iloc[train_idxs], data_df.iloc[val_idxs]\n","    train_idxs, test_idxs = next(GroupShuffleSplit(test_size=(0.6/0.8), n_splits=2, random_state = 42).split(train_df, groups=train_df['Selected_Service_ID']))    \n","    train_df, test_df = train_df.iloc[train_idxs], train_df.iloc[test_idxs]\n","    y_train, y_val, y_test = train_df.pop('Label'), val_df.pop('Label'), test_df.pop('Label')\n","    train_df, val_df, test_df = train_df[CSV_COLUMNS_SEL], val_df[CSV_COLUMNS_SEL], test_df[CSV_COLUMNS_SEL]\n","    return train_df.values, val_df.values, test_df.values, y_train.values, y_val.values, y_test.values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-unuEDewVX_Z","colab_type":"code","colab":{}},"source":["X_train, X_val, y_train, y_val = load_df(data_files)\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=(0.6/0.8), random_state=19, shuffle=True)\n","\n","# import dask.array as da\n","# dtypes = [np.float32,np.float32,str,str,str,np.array([]),str,str]\n","# dtype_ex = pd.Series([0.0,0.0,'','','',[],'',''])\n","\n","# convert back to df's for easier data transformation\n","X_tr = dd.from_dask_array(da.from_array(X_train, chunks=(2000,8)))\n","X_tr.columns = CSV_COLUMNS_SEL\n","# X_tr['Shared_Recommendations'] = X_tr['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","X_vl = dd.from_dask_array(da.from_array(X_val, chunks=(2000,8)))\n","X_vl.columns = CSV_COLUMNS_SEL\n","# X_vl['Shared_Recommendations'] = X_vl['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","X_te = dd.from_dask_array(da.from_array(X_test, chunks=(2000,8)))\n","X_te.columns = CSV_COLUMNS_SEL\n","# X_te['Shared_Recommendations'] = X_te['Shared_Recommendations'].apply(lambda x: pd.arrays.SparseArray(x).astype(np.int32), meta=dtype_ex).compute()\n","\n","print(\"Using train dataset with shape: {}\".format(X_tr.shape))\n","print(\"Using validation dataset with shape: {}\".format(X_vl.shape))\n","print(\"Using test dataset with shape: {}\".format(X_te.shape))\n","X_tr.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiwx55CU-nuD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z5SUTduRVUu","colab_type":"code","colab":{}},"source":["CSV_COLUMNS = [\n","            'Distance',\n","            'Embedding_Similarity',\n","            'Month',\n","            'Selected_Service_Embedding',\n","            'Candidate_Service_Embedding', \n","            'Previously_Recommended',\n","            'Query',\n","            'Selected_Service_ID',\n","            'Candidate_Service_ID'\n","]\n","\n","# following: https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n","\n","# initialize inputs for model with appropriate keras layers\n","wide_in = {}\n","deep_in = {}\n","for i in range(len(CSV_COLUMNS)):\n","    if i in (0,1):\n","        print(\"Wide Input: Added {} input layer as float32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.float32)\n","        # inputs[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype='float32')\n","    elif i == 2:\n","        print(\"Wide Input: Added {} input layer as int32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.int32)\n","    elif i in (3,4,6):\n","        print(\"Deep Input: Added {} input layer as string\".format(CSV_COLUMNS[i]))\n","        deep_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(), dtype=tf.string)\n","        # inputs[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(512,), dtype='float32')\n","    # elif i == 5:\n","    #     print(\"Added {} input layer as list of strings\".format(CSV_COLUMNS[i]))\n","    #     wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(16547,), dtype='string')\n","        # inputs[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(16547,), dtype='int32')\n","    elif i in (7,8):\n","        print(\"Wide Input: Added {} input layer as int32\".format(CSV_COLUMNS[i]))\n","        wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype=tf.int32)\n","    # else:\n","    #     print(\"Added {} input layer as string\".format(CSV_COLUMNS[i]))\n","    #     wide_in[CSV_COLUMNS[i]] = tf.keras.layers.Input(name=CSV_COLUMNS[i], shape=(1,), dtype='string')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bn0DglpYv9pv","colab_type":"code","colab":{}},"source":["print(wide_in)\n","print(deep_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRozssyS8AOj","colab_type":"code","colab":{}},"source":["# import tensorflow_text\n","\n","# Load the TF-Hub module to download model files if not present and to def\n","# embedding funciton if needed\n","print(\"Loading the TF-Hub {} module...\".format(MODEL))\n","print(\"TF-Hub module is loaded.\")\n","\n","# pre-load to download files if not present\n","embed_fn = hub.load(MODEL_URL)\n","\n","def extract_embed(text):\n","\n","    # import tensorflow_hub as hub\n","    # text = tf.reshape(text, [-1])\n","    # return embed_fn.signatures[\"default\"](tf.squeeze(tf.cast(text, tf.string)))['default']\n","    return embed_fn(text)#['default']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8zSC4fPVuMG","colab_type":"code","colab":{}},"source":["# import random\n","MODEL_DIR = os.path.join(PREFIX, 'models', MODEL_TYP, \"init_baseline_deep-only\")#.format(BATCH_SIZE)) #+ str(random.randint(1,1001))\n","print(\"Using model dir {}\".format(MODEL_DIR))\n","\n","config = tf.estimator.RunConfig(model_dir=MODEL_DIR, tf_random_seed=42, save_summary_steps=100,\n","    save_checkpoints_steps=250, session_config=None, keep_checkpoint_max=50, \n","    keep_checkpoint_every_n_hours=1,session_creation_timeout_secs=7200)\n","\n","def rec_line_gen(R):\n","    for r in range(len(R)):\n","        print(R.shape)\n","        print(r)\n","        yield np.asarray(R[r])\n","\n","def input_generator(X,y):\n","    while True:\n","        X = np.array(X)\n","        for i in range(len(X)):\n","            # print(X[i].shape)\n","            # WIDE-DEEP:\n","            x1,x2,x3,x4,x5,x6,x7 = X[:,i]\n","            # WIDE-DEEP:\n","            yield np.float32(x1),np.float32(x2),str(x3),str(x4),str(x5),str(x6),np.str(x7),y[i]\n","\n","def input_fn(X, y, shuffle=True):\n","    \n","    def parse_line(f1,f2,f3,f4,f5,f6,f7,f8,f9):\n","        cols = [f1,f2,f3,f4,f5,f6,f7,f8,f9]\n","        lbl = cols.pop(-1)\n","        labels = {'Pred': lbl}\n","\n","        feats = dict(zip(CSV_COLUMNS, cols))\n","        return feats,labels\n","\n","    dist = X['Distance'].values.astype(np.float32)\n","    sim = X['Embedding_Similarity'].values.astype(np.float32)\n","    mon = X['Month'].values.astype(np.int32)\n","    sel = X['Selected_Service_Embedding'].values\n","    cand = X['Candidate_Service_Embedding'].values\n","    # recs = X['Previously_Recommended'].apply(lambda x: np.array(np.asarray(x))).values\n","    # global max_rec_len\n","    # ds6 = tf.data.Dataset.from_tensor_slices((X['Previously_Recommended'].values))\n","    query = X['Query'].values.astype(str)\n","    sel_sid = X['Selected_Service_ID'].values.astype(np.int32)\n","    cand_sid = X['Candidate_Service_ID'].values.astype(np.int32)\n","\n","    ds1 = tf.data.Dataset.from_tensor_slices(dist)\n","    ds2 = tf.data.Dataset.from_tensor_slices(sim)\n","    ds3 = tf.data.Dataset.from_tensor_slices(mon)\n","    ds4 = tf.data.Dataset.from_tensor_slices(sel)\n","    ds5 = tf.data.Dataset.from_tensor_slices(cand)\n","    # ds6 = tf.data.Dataset.from_tensor_slices(recs)\n","    ds7 = tf.data.Dataset.from_tensor_slices(query)\n","    ds8 = tf.data.Dataset.from_tensor_slices(sel_sid)\n","    ds9 = tf.data.Dataset.from_tensor_slices(cand_sid)\n","    ds10 = tf.data.Dataset.from_tensor_slices(y)\n","    \n","    dataset = tf.data.Dataset.zip((ds1,ds2,ds3,ds4,ds5,ds7,ds8,ds9,ds10))\n","    dataset = dataset.map(parse_line, num_parallel_calls=4)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(X))\n","        dataset = dataset.repeat(TRAIN_EPOCHS)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    # keeps dataset in Memory. dangerous if large but req for TPU\n","    # dataset = dataset.cache()\n","    return dataset\n","\n","# ds =  input_fn(X_tr, y_train)\n","# feed_dict = {}\n","# for col in CSV_COLUMNS_SEL:\n","#     feed_dict[col] = X_tr[col].values\n","# feed_dict.update({'Pred': y_train})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J_mli2NKacP","colab_type":"code","colab":{}},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","\n","# build wide and deep component layers and combine their outputs for classification\n","def wide_and_deep_classifier(wide_in, deep_in, linear_feature_columns, dnn_feature_columns, dnn_hidden_units):\n","\n","    \n","    # following: https://blog.tensorflow.org/2018/04/predicting-price-of-wine-with-keras-api-tensorflow.html\n","    \n","    wide = layers.DenseFeatures(linear_feature_columns)(wide_in)\n","    wide_model = Model(inputs=list(wide_in.values()), outputs=wide)\n","    wide_model.compile(loss='mse', optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.01,l1_regularization_strength=1.0), metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","\n","    # # create embedding layer \n","    \n","    sel_emb_in ,cand_emb_in, q_emb_in = deep_in.values()\n","    embed_layer = hub.KerasLayer(MODEL_URL, input_shape=[], dtype=tf.string, trainable=False)\n","\n","    sel_emb = embed_layer(sel_emb_in)\n","    # sel_emb = layers.Flatten(sel_emb)\n","    cand_emb = embed_layer(cand_emb_in)\n","    # cand_emb = layers.Flatten()(cand_emb_in)\n","    q_emb = embed_layer(q_emb_in)\n","    # q_emb = layers.Flatten()(q_emb_in)\n","\n","    embs = layers.concatenate([sel_emb,cand_emb,q_emb])\n","    deep = layers.Dense(512, activation='relu')(embs)\n","    # deep = layers.BatchNormalization(deep)\n","    for numnodes in dnn_hidden_units:\n","        deep = layers.Dense(numnodes, activation='relu')(deep)\n","        deep = layers.Dropout(0.2)(deep)\n","    # deep = layers.BatchNormalization()(deep)\n","    deep_model = Model(inputs=[sel_emb_in, cand_emb_in, q_emb_in], outputs=deep)\n","    deep_model.compile(optimizer='Adagrad', loss='binary_crossentropy', metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","\n","    pre_preds = layers.concatenate([wide_model.output, deep_model.output])\n","    pred = layers.Dense(1, activation='sigmoid', name='Pred')(pre_preds)\n","    wide_n_deep_model = Model(inputs=[wide_model.inputs + deep_model.inputs], outputs=pred)\n","    wide_n_deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","\n","    return wide_n_deep_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBuENTrDTF57","colab_type":"code","colab":{}},"source":["# MODEL_URL = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n","wide_columns, deep_columns = build_model_columns()\n","print(wide_columns)\n","# wide_columns, deep_columns = None,None\n","hidden = [256,128]\n","model = wide_and_deep_classifier(wide_in, deep_in, wide_columns, deep_columns, hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0pQm7IGWWzV","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkBeTEGMbuDv","colab_type":"code","colab":{}},"source":[" tf.keras.utils.plot_model(model, os.path.join(PREFIX, 'figures', 'deep_keras_model.png'), show_shapes=True, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSJ_yz5o4IMl","colab_type":"code","colab":{}},"source":["# with strategy.scope():\n","    # wide_n_deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','AUC', 'Recall', 'Precision'])\n","\n","col_types = [np.float32,np.float32,np.int32,str,str,str,str,np.int32,np.int32]\n","train_dict = {}\n","for i, col in enumerate(CSV_COLUMNS_SEL):\n","    train_dict[col] = X_tr[col].values.astype(col_types[i])\n","val_dict = {}\n","for i,col in enumerate(CSV_COLUMNS_SEL):\n","    val_dict[col] = X_vl[col].values.astype(col_types[i])\n","train_ds = input_fn(X_tr, y_train)\n","\n","# model_hist = model.fit(train_ds, batch_size=BATCH_SIZE, epochs=TRAIN_EPOCHS, steps_per_epoch=len(X_train)//BATCH_SIZE,verbose=10, validation_data=(list(val_dict.values()), y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gn8Efd7-i14n","colab_type":"code","colab":{}},"source":["model_hist = model.fit(train_ds, batch_size=BATCH_SIZE, epochs=TRAIN_EPOCHS, steps_per_epoch=len(X_train)//BATCH_SIZE,verbose=10, validation_data=(list(val_dict.values()), y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj8EgobRELIl","colab_type":"code","colab":{}},"source":["from plot_keras_history import plot_history\n","import matplotlib.pyplot as plt\n","\n","# plot keras history metrics \n","plot_history(model_hist.history)\n","plt.show()\n","plot_history(model_hist.history, path=\"standard.png\")\n","plt.close()\n","\n","# follow for full model report\n","# https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n","# https://www.machinecurve.com/index.php/2019/10/08/how-to-visualize-the-training-process-in-keras/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1j8xmV0bnIE","colab_type":"code","colab":{}},"source":["feed_dict = {}\n","for col in DEEP_COLUMNS:\n","    feed_dict[col] = X_te[col].values\n","# feed_dict.update({'Pred': y_train})\n","model.evaluate(x=list(feed_dict.values()), y=y_test, batch_size=BATCH_SIZE, verbose=1, use_multiprocessing=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fEnWQdRZmRZ","colab_type":"code","colab":{}},"source":["MODEL_TYP = 'wide-deep-USE'\n","MODEL_DIR = os.path.join(PREFIX, 'models', 'wide_and_deep_{}'.format(MODEL), MODEL_TYP)\n","model_fpath = os.path.join(MODEL_DIR, '{}-model.tf'.format(MODEL_TYP))\n","model.save(model_fpath, save_format='tf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hU0xWRDVqbW","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir \"$MODEL_DIR/\" #--debugger_port 6969 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XU8lVbF2SkzK","colab_type":"code","colab":{}},"source":["wcols, dcols = build_model_columns()\n","wide_deep = tf.compat.v1.estimator.DNNLinearCombinedClassifier(\n","    model_dir=None, linear_feature_columns=wcols, linear_optimizer='Ftrl',\n","    dnn_feature_columns=dcols, dnn_optimizer='Adagrad', dnn_hidden_units=[512,256,128],\n","    dnn_activation_fn=tf.nn.relu, dnn_dropout=0.2, n_classes=2, weight_column=None,\n","    label_vocabulary=None, config=None, warm_start_from=None,\n","    batch_norm=True,linear_sparse_combiner='sum')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MIFUYocrONQ","colab_type":"code","colab":{}},"source":["train_spec = tf.compat.v1.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","eval_spec = tf.compat.v1.estimator.EvalSpec(input_fn=lambda: input_fn(X_vl, y_val), name='wide-deep-ref-impl_bs_{}'.format(BATCH_SIZE),\n","                                  start_delay_secs=2, throttle_secs=10)\n","# evals, exports = tf.compat.v1.estimator.train_and_evaluate(wide_deep, train_spec, eval_spec)\n","wide_deep.train(lambda: input_fn(X_tr, y_train), steps=len(X_tr)//BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8l-RCB_0aW2I","colab_type":"code","colab":{}},"source":["    # model = tf.keras.models.load_model(model_fpath)\n","    config = tf.estimator.RunConfig(model_dir=MODEL_DIR, tf_random_seed=19, save_summary_steps=100,\n","        save_checkpoints_secs=120,session_config=None, keep_checkpoint_max=5,\n","        keep_checkpoint_every_n_hours=2,log_step_count_steps=100, \n","        train_distribute=None, device_fn=None, protocol=None, eval_distribute=None,\n","        experimental_distribute=None,experimental_max_worker_delay_secs=None,\n","        session_creation_timeout_secs=7200)\n","\n","    model_estimator = tf.keras.estimator.model_to_estimator(model, model_dir=MODEL_DIR, config=config)\n","    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(X_tr, y_train))\n","    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(X_te, y_test), name='wide-deep-care-net_batch-size_{}'.format(BATCH_SIZE),\n","                                    start_delay_secs=2, throttle_secs=10)\n","    evals, exports = tf.estimator.train_and_evaluate(model_estimator, train_spec, eval_spec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsvjtg-FtKcK","colab_type":"code","colab":{}},"source":["X_tr['Selected_Service_Embedding'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zI5maBHdyBbv","colab_type":"code","colab":{}},"source":["import math\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","# train_ds = input_fn(X_tr, y_train)\n","# test_ds = input_fn(X_te, y_test, shuffle=False)\n","num_5k_steps = math.ceil(len(X_te) / 5000) \n","train_steps = math.ceil(len(X_tr) / num_5k_steps)\n","\n","accs, precs, recs, f1s = [],[],[],[]\n","for i in range(num_5k_steps):\n","    print('='*20, \"Train/Eval round {}/{}\".format(i,num_5k_steps), '='*20)\n","    eval_er.train(input_fn=lambda: input_fn(X_tr, y_train), steps=train_steps)\n","\n","    start_idx = i * train_steps\n","    end_idx = start_idx + train_steps\n","    print(\"Using train samples {} to {}\".format(start_idx,end_idx))\n","    X_metr = None\n","    preds = None\n","    y_metr = None\n","    if end_idx <= len(X_train):\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx:end_idx]\n","        print(\"Using subset of train data with size {}\".format(len(X_metr)))\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx:end_idx]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    else:\n","        X_metr = pd.DataFrame(X_train).iloc[start_idx::]\n","        X_metr.columns = CSV_COLUMNS\n","        y_metr = y_train[start_idx::]\n","        preds = eval_er.predict(input_fn=lambda: input_fn(X_metr, y_metr))\n","    yhat = []\n","    for i in range(train_steps):\n","        pred = next(preds)\n","        print(pred.keys())\n","        yhat.append(pred['Pred'])\n","    print(\"Going through {} predictions for training subset\".format(len(yhat)))\n","    yhat_classes = list(map(lambda x: np.round(x),yhat))\n","    # accuracy: (tp + tn) / (p + n)\n","    accuracy = accuracy_score(y_metr, yhat_classes)\n","    print('Accuracy: %f' % accuracy)\n","    # precision tp / (tp + fp)\n","    precision = precision_score(y_metr, yhat_classes)\n","    print('Precision: %f' % precision)\n","    # recall: tp / (tp + fn)\n","    recall = recall_score(y_metr, yhat_classes)\n","    print('Recall: %f' % recall)\n","    # f1: 2 tp / (2 tp + fp + fn)\n","    f1 = f1_score(y_metr, yhat_classes)\n","    print('F1 score: %f' % f1)\n","    accs.append(accuracy)\n","    precs.append(precision)\n","    recs.append(recall)\n","    f1s.append(f1)\n","\n","    eval_er.evaluate(input_fn=lambda: input_fn(X_te, y_test, shuffle=False), steps=5000, name='5kEvals')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GR-lfvR_1up","colab_type":"code","colab":{}},"source":["    cfname = os.path.join(MODEL_DIR, 'label_metadata.tsv')\n","with open(fname, 'w') as f:\n","    f.write(\"Index\\tLabel\\tRecommended\\n\")\n","    sids = list(tagged_embeds.keys())\n","    for idx in range(len(sids)):\n","        typ = None\n","        if sids[idx] in hin_nodes:\n","            name = hin_nodes[sids[idx]]['name']\n","            typ = 'Service'\n","        else:\n","            name = sids[idx]\n","            typ = 'Query'\n","        # sel_name = serv_nodes[sel_idx]['name']\n","        # cand_idx = str(int(X_t['Candidate_Service_Embedding'].values[idx]))\n","        # try:\n","            # cand_name = serv_nodes[cand_idx]['name']\n","        # except:\n","            # continue\n","        # name = \"{}-{}\".format(sel_name, cand_name)\n","        f.write(\"{}\\t{}\\t{}\\n\".format(idx, name, typ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLTXPnxjaK-B","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","\n","preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# yhat = np.array([tf.argmax(pred['Pred'],1) for pred in preds])[:,0]\n","yhat = []\n","for pred in preds:\n","    yhat.append(pred['Pred'])\n","yhat_classes = list(map(lambda x: np.round(x),yhat))\n","\n","\n","# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(y_test, yhat_classes)\n","print('Accuracy: %f' % accuracy)\n","# precision tp / (tp + fp)\n","precision = precision_score(y_test, yhat_classes)\n","print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_test, yhat_classes)\n","print('Recall: %f' % recall)\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(y_test, yhat_classes)\n","print('F1 score: %f' % f1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"odCFG6HKQ-zx","colab_type":"code","colab":{}},"source":["import datetime\n","X_t = pd.DataFrame(X_test)\n","X_t.columns = CSV_COLUMNS\n","# X_t.columns = DEEP_COLUMNS\n","# X_t.columns = WIDE_COLUMNS #+ ['Selected_Service_ID']\n","\n","\n","preds = eval_er.evaluate(lambda: input_fn(X_t, y_test,shuffle=False), name='evalShuffled')\n","# preds = eval_er.predict(lambda: input_fn(X_t, y_test,shuffle=False))\n","# print(next(preds))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hh6r6hDtdEFv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVyEaLfOrnNp","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"AlBSM3DG_ClO","colab_type":"code","colab":{}},"source":["    !rm -rf MODEL_DIR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"reOkfht5yzyY","colab_type":"code","colab":{}},"source":["print(\"Loading model as keras model from {}\".format(export_path))\n","export_path = os.path.join('/tmp/census_model', \"1585107419\")\n","keras_model = tf.keras.models.load_model(export_path, compile=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2YQj0gN3tA0","colab_type":"code","colab":{}},"source":["print(keras_model.asset_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l82RDk-ApgrZ","colab_type":"code","colab":{}},"source":["keras_model.tensorflow_version\n","tf.keras.utils.plot_model(keras_model, 'census_test_model.png', show_shapes=False, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4vTaKAyVLqu","colab_type":"code","colab":{}},"source":["!ls \"$export_path/../\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok6gHp-Gzwoo","colab_type":"code","colab":{}},"source":["# FLAGS, unparsed = parser.parse_known_args()\n","# tf.app.run(argv=[sys.argv[0]] + unparsed)\n","\n","# if not tf.compat.v1.gfile.Exists(FLAGS.data_dir):\n","#     tf.compat.v1.gfile.MkDir(FLAGS.data_dir)\n","\n","# data_df = pd.DataFrame()\n","# data_df, labels = load_df(DATA_DIR)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2wvMcd5b7xY","colab_type":"code","colab":{}},"source":["# dataset = complete_dataset(data_df)\n","# TEST_PCT = 0.3\n","# test_size = int(TEST_PCT * len(dataset))\n","# train_size = int(len(dataset) - test_size)\n","\n","# test_data = dataset.take(test_size)\n","# train_data = dataset.skip(test_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cxGgku62SPs","colab_type":"code","colab":{}},"source":["# def build_estimator(model_dir, model_type):\n","#   \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n","#   wide_columns, deep_columns = build_model_columns()\n","#   hidden_units = [100, 75, 50, 25]\n","\n","#   # Create a tf.estimator.RunConfig to ensure the model is run on GPU\n","#   run_config = tf.estimator.RunConfig().replace(\n","#       session_config=tf.compat.v1.ConfigProto(device_count={'GPU': 1}))\n","\n","#   if LEARN_TYPE == 'wide':\n","#     return tf.estimator.LinearClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=wide_columns,\n","#         config=run_config)\n","#   elif LEARN_TYPE == 'deep':\n","#     return tf.estimator.DNNClassifier(\n","#         model_dir=model_dir,\n","#         feature_columns=deep_columns,\n","#         hidden_units=hidden_units,\n","#         config=run_config)\n","#   else:\n","#     return tf.estimator.DNNLinearCombinedClassifier(\n","#         model_dir=model_dir,\n","#         linear_feature_columns=wide_columns,\n","#         dnn_feature_columns=deep_columns,\n","#         dnn_hidden_units=hidden_units,\n","#         config=run_config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eI4sWLGg2cxg","colab_type":"code","colab":{}},"source":["def input_fn(data_files, num_epochs, shuffle, batch_size):\n","#     \"\"\"Generate an input function for the Estimator.\"\"\"\n","#     # assert tf.compat.v1.gfile.Exists(data_file), (\n","#     #     '%s not found. Please make sure you have either run data_download.py or '\n","#     #     'set both arguments --train_data and --test_data.' % data_file)\n","\n","#     def parse_dataset_lines(value):\n","#         # print('Parsing dataset...')\n","#         columns = tf.io.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS, select_cols=[0,1,2,3,4,6,7])\n","#         print(columns)\n","#         print()\n","#         # refactor our data. TODO change this directly when creating dataset\n","#         lbl = columns.pop(-1)\n","#         labels = {'Label': tf.py_function(func=preproc_lbl,\n","#                                                 inp=[lbl],\n","#                                                 Tout=tf.int32)}\n","#         # labels = lbl\n","#         CSV_COLUMNS[-1] = 'Selected_Service_ID'\n","#         columns = columns + [columns[3]]\n","#         print(\"Pass1\")\n","#         features = dict(zip(CSV_COLUMNS, columns))\n","#         features.pop(CSV_COLUMNS[5])\n","#         features[CSV_COLUMNS[-1]] = columns[3]\n","#         print(\"Pass2\")\n","#         for i in (3,4):\n","#             feat = features[CSV_COLUMNS[i]]\n","#             features[CSV_COLUMNS[i]] = tf.py_function(func=preproc_serv,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_query,\n","#                                                       inp=[feat],\n","#                                                       Tout=(tf.float32))\n","#         print(features)\n","#         # features[CSV_COLUMNS[5]] = tf.py_function(func=preproc_rec,\n","#                                                             #  inp=[features[CSV_COLUMNS[5]]],\n","#         print(\"Pass3\")                                        #  Tout=(tf.int32))\n","#         return features, labels\n","        \n","#     # Extract lines from input files using the Dataset API.\n","#     # data_files = tf.io.matching_files(tf.convert_to_tensor(data_files))\n","    \n","#     dfs = tf.data.Dataset.from_tensor_slices(data_files)\n","#     # dataset = tf.data.TextLineDataset(data_files)\n","#     dataset = tf.data.TextLineDataset(data_files)\n","#     print(dataset)\n","#     if shuffle:\n","#         dataset = dataset.shuffle(buffer_size=TRAIN_SIZE)\n","#     dataset = dataset.map(parse_dataset_lines, num_parallel_calls=multiprocessing.cpu_count())\n","#     print(\"Pass4\")\n","#     # dataset = dataset.map(fn, num_parallel_calls=multiprocessing.cpu_count())\n","#     # dataset = dataset.map(parse_dataset, num_parallel_calls=multiprocessing.cpu_count())\n","\n","#     # We call repeat after shuffling, rather than before, to prevent separate\n","#     # epochs from blending together.\n","#     print(dataset)\n","#     dataset = dataset.repeat(TRAIN_EPOCHS)\n","#     dataset = dataset.batch(batch_size)\n","#     dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n","#     print(\"Pass5\")\n","#     return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6FOscOI2xyF","colab_type":"code","colab":{}},"source":["# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","# # Clean up the model directory if present\n","# shutil.rmtree(MODEL_DIR, ignore_errors=True)\n","# model = build_estimator(MODEL_DIR, LEARN_TYPE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5vKQtysDcjz","colab_type":"code","colab":{}},"source":["\n","# for n in range(TRAIN_EPOCHS // EPOCHS_PER_EVAL):\n","#     model.train(input_fn=lambda: input_fn(\n","#         data_files=train_files,\n","#         num_epochs=EPOCHS_PER_EVAL,\n","#         shuffle=True,\n","#         batch_size=BATCH_SIZE))\n","\n","#     results = model.evaluate(input_fn=lambda: input_fn(\n","#         data_files=test_files,\n","#         num_epochs=1,\n","#         shuffle=False,\n","#         batch_size=BATCH_SIZE))\n","\n","#     # Display evaluation metrics\n","#     print('Results at epoch', (n + 1) * EPOCHS_PER_EVAL)\n","#     print('-' * 60)\n","\n","#     for key in sorted(results):\n","#         print('%s: %s' % (key, results[key]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qsoPPX6qzDW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}